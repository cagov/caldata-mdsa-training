{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>CalData's Modern Data Stack training was born from our team's service called the Modern Data Stack Accelerator (MDSA). The service is a practical approach to help departments rapidly adopt modern cloud-based data tools while working on a data challenge they want to solve. In a nutshell the MDSA helps empower teams to use modern data tools effectively by demystifying what building a modern data stack means, including:</p> <ul> <li>architecting and procuring data stack components,</li> <li>creating a culture of data operations across a team,</li> <li>developing repeatable, automated, and observable processes, and</li> <li>staffing to maintain a modern data stack.</li> </ul> <p>The accelerator starts with a real business problem where supporting data has traditionally been difficult to combine and/or clean up. Using modern data tools in ODI's environment, combined with training and consultation, teams learn how to transform and automate this process. The goal of the service is to build a team's capacity to use the modern data stack and be ready to move from demonstration to production. You can read more about the service on our team's site.</p> <p>As we have now successfully trained several state departments on this service, this website serves as our attempt to open source our resources. We hope this enables anyone -- regardless of what tools they have in their stack -- to learn data and analytics engineering concepts and skills.</p>"},{"location":"cloud-data-warehouses/snowflake/","title":"Snowflake training","text":""},{"location":"cloud-data-warehouses/snowflake/#training-overview","title":"Training overview","text":"<p>This training is intended to give a very high overview of how to interact with key functionality in Snowflake. The training is split into two days, each day has approx. 90 minutes of content (a mix of lecture, live demonstration, and interactive exercises)</p> <ul> <li> <p>Day 1 covers an intro to Snowflake, how your team will be using it, some of the underlying architecture of Snowflake in general and specific to how ODI usually sets up data projects, and a tour of the Snowflake user interface</p> </li> <li> <p>Day 2 covers the basics of data loading, storage, and querying in Snowflake</p> </li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#day-1","title":"Day 1","text":""},{"location":"cloud-data-warehouses/snowflake/#what-is-snowflake","title":"What is Snowflake?","text":"<p>Snowflake is a cloud-based data platform that allows organizations to store and analyze large volumes of data. It provides a scalable and flexible solution for data warehousing, enabling users to efficiently manage, share, and query their data using a unique multi-cluster, shared architecture. Snowflake is designed to simplify data management, provide collaboration, and support advanced analytics in a secure and cost-effective manner.</p>"},{"location":"cloud-data-warehouses/snowflake/#how-snowflake-fits-into-our-project","title":"How Snowflake fits into our project","text":"<p>This is how Snowflake fits into the modern data stack we will be building together:</p> <ul> <li>Snowflake is where your data will be stored. We will be gathering all the data for your project and storing it in the RAW tables in your Snowflake account</li> <li>When we get into dbt later on, you\u2019ll see how we use dbt to modify the raw data that is in Snowflake to transform and join data together, to build analyses / dashboards off of</li> <li>Both the raw data and the transformed data (from dbt) will be stored here in Snowflake.</li> </ul> <p>Some examples of how you might be using Snowflake:</p> <ul> <li>If you are an owner of one of the data sources, you\u2019ll need to know what the raw data coming into Snowflake should look like, and whether it has landed here correctly. You might set up the data connectors that move data into Snowflake, and you might log into Snowflake to examine and explore the data to ensure the data looks accurate and complete.</li> <li>If you are a project owner or a systems administrator, you might log into Snowflake to understand storage and query costs, give or revoke access to users, or answer any data security questions about the project.</li> <li>If you are an analytics engineer, you will be doing most of your work to transform the data in dbt, but you should understand that whatever data you query and all the queries you build will actually be running in Snowflake.<ul> <li>You\u2019ll also want to come into Snowflake to look at query history for query optimization.</li> <li>You can explore and take a look at data, column names and types, etc. to better understand how to build the final datasets.</li> </ul> </li> <li>If you are a data engineer, you might use Snowflake's query history to investigate errors in the data pipeline that dbt can not explain.</li> <li>If you are an analyst, you can use Snowflake to run adhoc queries of the data and get instant results. You can use SQL and/or Python to examine the data and look for patterns.</li> <li>If you are building reports or dashboards, you will be pulling the data for those directly from Snowflake.</li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#snowflake-architecture","title":"Snowflake architecture","text":"<p>Snowflake's architecture differs from a traditional transactional database in that it separates storage and compute resources. It consists of three layers:</p> <p> Image source</p> <ol> <li> <p>Cloud services</p> <ul> <li>Infrastructure manager: manages the set up, monitoring, and maintenance of the Snowflake environment</li> <li>Optimizer: an intelligent query optimizer that decides the most efficient way to execute queries</li> <li>Transaction Manager: maintains metadata, schema, and query execution plans</li> <li>Security: robust security features including role-based access control, user authentication, and encryption</li> </ul> </li> <li> <p>Compute resources (for query processing)</p> <ul> <li>Executes queries using Massive Parallel Processing, a method that allows big jobs to be split up into smaller ones and processed at the same time, bringing down overall runtime</li> <li>Each query runs in independent virtual warehouses, ensuring high performance and isolation of workloads. Note: Snowflake uses the term virtual warehouse to refer to a cluster of compute resources, even though in many other contexts, virtual warehouse refers to the entire database!</li> <li>Virtual warehouses can expand or contract based on the complexity and size of queries</li> </ul> </li> <li> <p>Storage resources</p> <ul> <li>Data is stored in a columnar format</li> <li>Data is saved in blobs (binary large objects) on cloud storage</li> <li>Automatic compression</li> </ul> </li> </ol> <p>This architecture is what makes Snowflake more flexible (and arguably more user-friendly) than other solutions, because it separates resources needed for data storage from resources needed for accessing and processing data. This means that scaling your data storage needs up or down can be done independently of scaling the compute power needed for your queries.</p> <p>Summary of Advantages:</p> <ol> <li>Separation of Compute and Storage: Ensures scalability and cost efficiency.</li> <li>Ease of Use: Minimal infrastructure management with fully automated features.</li> <li>High Performance: Thanks to MPP and the ability to spin up multiple warehouses for concurrent workloads.</li> <li>Cloud Agnostic: Works seamlessly across multiple cloud providers.</li> <li>Secure and Reliable: Strong security protocols, encryption, and metadata management.</li> </ol>"},{"location":"cloud-data-warehouses/snowflake/#usage-based-pricing","title":"Usage-based pricing","text":"<p>With most on-premise transactional warehouses, costs scale with the number of server instances you buy and run. These servers usually are always-on and power various applications with high availability. In a traditional transactional warehouse both compute power and storage are associated with the same logical machine.</p> <p>Cloud data warehouses typically have a different pricing model: they decouple storage and compute and charge based on your query usage. Snowflake charges based on the amount of compute resources needed to execute your queries. Google BigQuery charges based on the amount of data your queries scan. There are also costs associated with data storage, but those are usually small compared to compute. Though these two models are slightly different, they both lead to a similar take-home lesson: by being careful with how data are laid out and accessed, you can significantly reduce both execution time and cost for your cloud data warehouses.</p>"},{"location":"cloud-data-warehouses/snowflake/#note-on-snowflake-sql","title":"Note on Snowflake SQL","text":"<p>The SQL Command Reference. From here, you can view all the SQL commands supported by Snowflake. The \u201cAll Commands\u201d entry provides an alphabetical list where you can find specific commands and click on them to be taken to the documentation page for that command.</p>"},{"location":"cloud-data-warehouses/snowflake/#tour-of-snowflake-user-interface","title":"Tour of Snowflake user interface","text":"<ol> <li>We\u2019ll give you a brief overview of the Snowflake user interface:<ol> <li>Home</li> <li>User menu Where you will find your username and the role that you currently have set. You will learn more about roles later on. This menu also allows you to switch to other accounts, access Snowflake documentation, and view your user profile.</li> <li>Data Where you can view all the data, including the raw, transformation, and final analytics layers (we will go over these later!).</li> <li>Projects Where you can query your data using SQL, Python, or Streamlit. Most of the code we will be writing to transform data will be done in dbt, but Snowflake is a great place to explore your data and build ad hoc analyses.</li> <li>Monitoring Where you can view query history and other logs, explore various data governance stats, and review security recommendations.</li> <li>Admin Where you can review resource usage, and where account admins can manage warehouses, users, roles, and security policies.</li> </ol> </li> </ol>"},{"location":"cloud-data-warehouses/snowflake/#snowflake-account-structure","title":"Snowflake account structure","text":"<p>Let's take a look at the hierarchy of all the different objects that make up your Snowflake project: </p>"},{"location":"cloud-data-warehouses/snowflake/#users-and-roles","title":"Users and roles","text":"<p>In Snowflake, every user is assigned one or more roles. Privileges are granted to roles, and this determines:</p> <ul> <li>What databases the role can see</li> <li>What virtual warehouses the role can use</li> <li>What permissions the role has on database objects (e.g., read-only, read-write, etc)</li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#databases-and-schemas","title":"Databases and schemas","text":"<p>An account can have many databases, used to logically organize data.</p> <p>A database contains schemas:</p> <ul> <li>Each database belongs to a single account</li> <li>Each schema belongs to a single database</li> </ul> <p>A schema is a logical grouping of objects:</p> <ul> <li>Tables, views, stages, file formats, etc</li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#virtual-warehouses","title":"Virtual warehouses","text":"<p>A virtual warehouse, usually referred to as a \"warehouse\" in Snowflake is a powerful and scalable computing resource that allows you to execute SQL queries and perform analytical tasks on your data. Unlike traditional data warehouses, Snowflake\u2019s virtual warehouses are \u201cvirtual\u201d in the sense that they are not tied to a specific hardware. Instead, they operate in the cloud and can dynamically allocate or deallocate computing resources based on your workload.</p> <p>A warehouse provides the required resources, such as CPU, memory, and temporary storage, to perform the following operations in a Snowflake session:</p> <ul> <li>Executing SQL SELECT statements that require compute resources (e.g. retrieving rows from tables and views)</li> <li>Performing DML (Data Manipulation Language) operations, such as:<ul> <li>Updating rows in tables (DELETE , INSERT , UPDATE)</li> <li>Loading data into tables (COPY INTO <code>table</code>)</li> <li>Unloading data from tables (COPY INTO <code>location</code>)</li> </ul> </li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#warehouse-sizes","title":"Warehouse sizes","text":"<p>Warehouses are available in a few different sizes, depending upon the needs of the data processing job:</p> <ul> <li>X-small (denoted by (XS)) Good for small tasks and experimenting.</li> <li>Small (denoted by S), Suitable for single-user workloads and development.</li> <li>Medium (denoted by M), Handles moderate concurrency and data volumes.</li> <li>Large denoted by (L), Manages larger queries and higher concurrency.</li> <li>X-Large (denoted by (XL)) Powerful for demanding workloads and data-intensive operations.</li> <li>2X-Large (denoted by 2XL)</li> <li>3X-Large (denoted by 3XL)</li> <li>4X-Large (denoted by 4XL)</li> </ul> <p>Note</p> <p>Note: Most jobs on small data should use the relevant X-small warehouse. Warehouse size affects the cost of your query, so err on the side of using the smallest warehouse to do your job. Snowflake documentation on warehouses</p>"},{"location":"cloud-data-warehouses/snowflake/#snowflake-context","title":"Snowflake context","text":"<p>Setting your context in Snowflake means establishing the specific environment or workspace within the platform where you will be working. It tells the platform where you want to perform your data operations. This includes setting the database, schema, and other parameters that define the scope. Setting your context ensures that you are directing your actions to the appropriate database and schema, preventing unintentional modifications or queries in the wrong location.</p> <p></p>"},{"location":"cloud-data-warehouses/snowflake/#exercise-set-your-context","title":"Exercise: Set your context","text":""},{"location":"cloud-data-warehouses/snowflake/#set-your-context-using-the-user-interface-instructions","title":"Set your context using the user interface instructions","text":"<ol> <li>Change your role to <code>public</code></li> <li>Navigate to Data &gt; Databases -- notice that nothing is here!</li> <li>Now change your role to <code>TRANSFORMER_DEV</code></li> <li>Navigate to <code>RAW_DEV</code> -&gt; <code>WATER_QUALITY</code></li> <li>View the Schema Details and Tables on this database</li> <li>Now change your role to <code>TRANSFORMER_PRD</code> -- how did this change the databases you can see?</li> </ol>"},{"location":"cloud-data-warehouses/snowflake/#set-your-context-using-sql-commands-instructions","title":"Set your context using SQL commands instructions","text":"<ol> <li>Navigate to Projects</li> <li>Click on the + button in the top right corner -- this should open a new Snowflake Worksheet</li> <li>Select the <code>RAW_DEV</code> database -- oops! that database is no longer available because we are in the <code>TRANSFORMER_PRD</code> role</li> <li> <p>Copy the following code into the worksheet, click the arrow next to the blue run button at the top of the screen, and select \"Run All\"</p> <pre><code>USE ROLE transformer_dev;\nUSE WAREHOUSE transforming_xs_dev;\nUSE DATABASE raw_dev;\nUSE SCHEMA water_quality;\nSHOW tables;\n</code></pre> </li> <li> <p>Notice you can also make these changes by selecting the Role and Warehouse at the top of the UI by the run button</p> </li> </ol>"},{"location":"cloud-data-warehouses/snowflake/#context-is-key","title":"Context is key","text":"<p>As you may have noticed from the exercises above, it can be really easy to have the wrong role selected when navigating in Snowflake.</p> <p>Pay attention to which role and warehouse you have active, especially if you cannot find a database or table you expect to see or you receive an \"object not found\" error. You might just need to change your role!</p>"},{"location":"cloud-data-warehouses/snowflake/#defaults","title":"Defaults","text":"<p>Every user will be assigned a default Role and Warehouse.</p> <p>Your default ROLE should be:</p> <ul> <li>TRANSFORMER_DEV: This is the analytics engineer/dbt role, for transforming raw data into something analysis-ready. It has read/write/control access to both TRANSFORM and ANALYTICS, and read access to RAW.</li> </ul> <p>Your default WAREHOUSE should be:</p> <ul> <li>TRANSFORMING_XS_DEV: This warehouse is for transforming data in TRANSFORM and ANALYTICS. For most small jobs, you want XS. It gets very costly as you scale up to larger warehouses. If your job is large (it takes a while or times out to run in XS?) or it\u2019s a long script, you can scale up, but maybe start in XS.</li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#odi-snowflake-architecture","title":"ODI Snowflake Architecture","text":"<p>We\u2019ve talked about Snowflake architecture generally. Now we\u2019re going to start talking about ODI-specific architecture. The setup of our account is adapted from the approach described in this dbt blog post, which we summarize here.</p> <p>There are two environments set up for this project, development and production. Resources in the development environment are suffixed with DEV, and resources in the production environment are suffixed with PRD.</p> <p>Most of the time, developers will be working in the development environment. Once your branches are merged to <code>main</code>, they will be used in the production environment.</p> <p>What follows is a brief description of the most important Snowflake resources in the dev and production environments and how developers are likely to interact with them.</p> <p>We'll get into all of this throughout the training (and throughout our engagement with you!) -- so don't feel you need to memorize this just yet.</p>"},{"location":"cloud-data-warehouses/snowflake/#six-databases","title":"Six databases","text":"<p>We have six primary databases in our project.</p> <p>Where our Source data lives</p> <pre><code>RAW_DEV: Development space for loading new source data.\nRAW_PRD: Landing database for production source data.\n</code></pre> <p>Where data from our Staging and Intermediate models lives</p> <pre><code>TRANSFORM_DEV: Dev space for staging/intermediate models. This is where most of your dbt work is!\nTRANSFORM_PRD: Prod space for models. This is what builds in the nightly job.\n</code></pre> <p>Where data from our Marts models lives</p> <pre><code>ANALYTICS_DEV: Dev space for mart models. Use this when developing a model for a new dashboard or report!\nANALYTICS_PRD: Prod space for mart models. Point production dashboards and reports to this database.\n</code></pre> <p>We will review the difference between Source, Staging/Intermediate, and Marts models in more depth during the dbt training!</p>"},{"location":"cloud-data-warehouses/snowflake/#six-warehouse-groups","title":"Six warehouse groups","text":"<p>There are six warehouse groups for processing data in the databases, corresponding to the primary purposes of the above databases. They are available in a few different sizes, depending upon the needs of the the data processing job.</p> <pre><code>LOADING_{size}_DEV: This warehouse is for loading data to RAW_DEV. It is used for testing new data loading scripts.\nTRANSFORMING_{size}_DEV: This warehouse is for transforming data in TRANSFORM_DEV and ANALYTICS_DEV. Most dbt developers will use this warehouse for daily work.\nREPORTING_{size}_DEV: This warehouse is for testing dashboards.\nLOADING_{size}_PRD: This warehouse is for loading data to RAW_PRD. It is used for production data loading scripts.\nTRANSFORMING_{size}_PRD: This warehouse is for transforming data in TRANSFORM_PRD and ANALYTICS_PRD. This warehouse is used for the nightly builds.\nREPORTING_{size}_PRD: This warehouse is for production dashboards.\n</code></pre>"},{"location":"cloud-data-warehouses/snowflake/#six-roles","title":"Six roles","text":"<p>There are six primary functional roles:</p> <pre><code>LOADER_DEV: Dev role for loading data to the RAW_DEV database. This is assumed when developing new data loading scripts.\nLOADER_PRD: Prod role for loading data to the RAW_PRD database. This is assumed by data loading scripts.\n\nTRANSFORMER_DEV: Dev role for transforming data. This is you! Models built with this role get written to the TRANSFORM_DEV or ANALYTICS_DEV databases. CI robots also use this role to run checks and tests on PRs before they are merged to main.\nTRANSFORMER_PRD: Prod role for transforming data. This is assumed by the nightly build job and writes data to the TRANSFORM_PRD or ANALYTICS_PRD databases.\n\nREPORTER_DEV: Dev role for reading marts. Use this when developing new dashboards. This role can read models in the ANALYTICS_DEV database.\nREPORTER_PRD: Prod role for reading marts. This is for users and service accounts using production dashboards. This role can read models in the ANALYTICS_PRD database.\n</code></pre>"},{"location":"cloud-data-warehouses/snowflake/#access-roles-vs-functional-roles","title":"Access Roles vs Functional Roles","text":"<p>In Snowflake, roles define which users have access to which data and what they can do with it. We create a two layer role hierarchy according to Snowflake's guidelines</p> <p>Access Roles are roles giving a specific access type (read, write, or control) to a specific database object, (e.g., \"read access on RAW\").</p> <p>Functional Roles represent specific user personae like \"developer\" or \"analyst\" or \"administrator\". Functional roles are built by being granted a set of Access Roles.</p>"},{"location":"cloud-data-warehouses/snowflake/#visualizing-the-odi-context","title":"Visualizing the ODI context","text":"<p>To make the preceding more concrete, let's consider the six databases, RAW, TRANSFORM, and ANALYTICS, for both DEV and PRD:</p> <p></p> <p>If you are a developer, you are doing most of your work in TRANSFORM_DEV and ANALYTICS_DEV, assuming the role TRANSFORMER_DEV. However, you also have the ability to select the production data from RAW_PRD for your development. But you do not have access to the production level data of the Transform and Analytics steps. So your data access looks like the following:</p> <p></p> <p>Again, some of this will make a lot more sense when we get further into the training. The main takeaway here is that your ability to view, transform, load, and create data in Snowflake is dependent on the exact combination of:</p> <ul> <li>your account -- determines which roles you have access to</li> <li>your role -- determines which databases you have access to</li> <li>your database -- determines what actions you can take (read, write)</li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#exercise","title":"Exercise:  <p>Let's play around with roles and databases a bit more</p> <ol> <li>In Snowflake, navigate to a new Worksheet</li> <li> <p>Assume the following context by running this code:</p> <pre><code>USE ROLE transformer_dev;\nUSE DATABASE transform_dev;\nUSE SCHEMA water_quality;\nUSE WAREHOUSE transforming_xs_dev;\n</code></pre> </li> </ol> <p>Using the TRANSFORMER_DEV role, we should have READ access to the TRANSFORM, ANALYTICS and RAW databases</p> <ol> <li> <p>Run the following code to read some data:</p> <pre><code>SELECT *\nFROM RAW_DEV.WATER_QUALITY.LAB_RESULTS\nLIMIT(20)\n</code></pre> </li> </ol>","text":""},{"location":"cloud-data-warehouses/snowflake/#day-2","title":"Day 2","text":""},{"location":"cloud-data-warehouses/snowflake/#loading-and-querying-data-in-snowflake","title":"Loading and querying data in Snowflake <p>On Day 1, we learned about cloud data warehouses, how Snowflake is structured, how to navigate the user interface, and ODI's recommended structure for warehouses, databases, and roles in Snowflake.</p> <p>Today we will work directly with data and learn about how to load data into Snowflake and how to query it.</p>","text":""},{"location":"cloud-data-warehouses/snowflake/#when-will-i-need-to-load-data-into-snowflake","title":"When will I need to load data into Snowflake? <p>During the set up phase of the MDSA project, ODI will be helping your team build data pipelines that load all your data from their source (whether that be on-prem SQL servers, online tools such as Google Analytics or Esri, cloud storage providers such as AWS S3 buckets, or even spreadsheets) into the RAW_PRD database in Snowflake on an automated schedule.</p> <p>This means that most of the data loading for this project will happen during the initial set up phase and this will not be something you need to do on a regular basis. However, it is helpful to understand how data is loaded into Snowflake.</p> <p>If you are a part of this initial set up phase, or need to load data in after the set up period (because you have a new datasource, or an old datasource needs to be connected in a new way), then you can find information about how to load data into Snowflake in the Snowflake documentation on this topic.</p>","text":""},{"location":"cloud-data-warehouses/snowflake/#exercise-1","title":"Exercise: 1 <ol> <li>Download the Stations data as a CSV</li> <li>In Snowflake, navigate to Data &gt; Add Data &gt; Browse and select the csv you downloaded and renamed</li> <li>Select the following:     Database: RAW_DEV     Schema: DATA_LOAD_TRAINING     Create new table     Name: [your first name]_stations</li> <li>Review load settings, then confirm</li> <li> <p>Press Query Data and explore the data. Use these queries as a starting point:</p> <pre><code>SELECT * FROM RAW_DEV.DATA_LOAD_TRAINING.[yourname]_stations LIMIT 10;\n\nSELECT * FROM RAW_DEV.DATA_LOAD_TRAINING.[yourname]_stations WHERE county_name = 'Yolo';\n\nSELECT station_type, COUNT(*) FROM RAW_DEV.DATA_LOAD_TRAINING.[yourname]_stations GROUP BY station_type;\n</code></pre> </li> </ol>","text":""},{"location":"cloud-data-warehouses/snowflake/#databases-tables-and-views","title":"Databases, tables, and views <p>All data in Snowflake is maintained in databases. Each database consists of one or more schemas, which are logical groupings of database objects, such as tables and views. Snowflake does not place any hard limits on the number of databases, schemas (within a database), or objects (within a schema) you can create.</p> <p>Much more information, including info on how Snowflake partitions and clusters in Snowflake's docs.</p> <p>We will dive deeper into the difference between tables and views during dbt training.</p>","text":""},{"location":"cloud-data-warehouses/snowflake/#tables","title":"Tables","text":"<p>All data in Snowflake is stored in database tables, logically structured as collections of columns and rows. To best utilize Snowflake tables, particularly large tables, it is helpful to have an understanding of the physical structure behind the logical structure.</p> <p>For any Snowflake table, you can open Data \u00bb Databases and search for or navigate to the table. Select the table to do any of the following:</p> <ul> <li>Explore details about the table and the columns defined in the table.</li> <li>Preview the data in the table.</li> <li>Load data into the table from files.</li> <li>Monitor the data loading activity for the table using the table-level Copy History.</li> </ul>"},{"location":"cloud-data-warehouses/snowflake/#views","title":"Views","text":"<p>A view allows the result of a query to be accessed as if it were a table. It is used when you need to query a result but don\u2019t need to contain the data permanently. Instead of storing the actual data, a view stores the query definition, allowing you to access the data dynamically when you query the view.</p> <p>It stores the SQL so you don\u2019t have to write it again and again and can reference it. But if the data is not needed all the time, it doesn\u2019t consume any add\u2019l storage. And it can encapsulate very complex queries.</p> <p>It can be modified without affecting the underlying tables that they\u2019re querying.</p>"},{"location":"cloud-data-warehouses/snowflake/#materialized-views","title":"Materialized Views","text":"<p>A materialized view is a pre-computed data set derived from a query and stored for later use.</p> <p>Querying a materialized view is faster than a view. They auto-refresh on a schedule that you define. But it does not refresh based on any changes in the data.</p>"},{"location":"cloud-data-warehouses/snowflake/#data-layout-columnar-storage","title":"Data layout: columnar storage <p>Most cloud data warehouses use columnar storage for their data. This means that data for each column of a table are stored sequentially in object storage (this is in contrast to transactional databases which usually store each row, or record, sequentially in storage). This BigQuery blog post goes into a bit more detail.</p> <p>On the left is data laid out in a record-oriented way, where each row's values are contiguous in memory. On the right is data laid out in a columnar way, where each column's values are contiguous in memory.</p> <p></p> <p>There are a number of advantages to using columnar storage for analytical workloads:</p> <ul> <li>You can read in columns separately from each other. So if your query only needs to look at one column of a several-hundred column table, it can do that without incurring the cost of loading and processing all of the other columns.</li> <li>Because the values in a column are located near each other in device storage, it is much faster to read them all at once for analytical queries like aggregations or window functions. In row-based storage, there is much more jumping around to different parts of memory.</li> <li>Having values of the same data type stored sequentially allows for much more efficient serialization and compression of the data at rest.</li> </ul>","text":""},{"location":"cloud-data-warehouses/snowflake/#data-layout-row-wise-partitions","title":"Data layout: row-wise partitions <p>In addition to columnar storage, cloud data warehouses also usually divide tables row-wise into chunks called partitions. Different warehouses choose different sizing strategies for partitions, but they are typically from a few to a few hundred megabytes.</p> <p>Having separate logical partitions in a table allows the compute resources to process the partitions independently of each other in parallel. This massively parallel processing capability is a large part of what makes cloud data warehouses like Snowflake scalable.</p>","text":""},{"location":"cloud-data-warehouses/snowflake/#review-tables-and-roles-from-day-1","title":"Review tables and roles from Day 1 <ul> <li>Explore the user interface again, focusing on Data, Roles, and Warehouses</li> </ul>","text":""},{"location":"cloud-data-warehouses/snowflake/#analyzing-queries-using-the-query-profile","title":"Analyzing Queries using the Query Profile <p>A Query profile refers to a detailed report or set of metrics generated for an executed query. The query profile provides insights into the query\u2019s performance, resource usage, and other relevant information. It is designed to help you identify potential performance bottlenecks and improvement opportunities.</p> <p>We\u2019ll give you a brief overview of how to investigate and debug issues with queries in Snowflake:</p> <ol> <li>Query details By looking at this, you can see if the query succeeded and if it ran within the time frame you were hoping for.<ol> <li>Query profile Note that there are two panels in the Query Profile that show specific aspects of execution:     -Most expensive nodes panel shows the nodes that took the longest to execute.     -Profile overview displays the percentage of execution time that went towards processing each stage of the queyr</li> <li>Statistics These figures tell you whether partition pruning is efficient or not.</li> <li>Data cache \u200b\u200bSnowflake caches data from queries you run so it can be accessed later by other queries. This cache is saved to disk in the virtual warehouse.</li> </ol> </li> </ol>","text":""},{"location":"cloud-data-warehouses/snowflake/#partition-pruning","title":"Partition pruning","text":"<p>Partition pruning is a process by which Snowflake eliminates partitions from a table scan based on the query\u2019s WHERE clause and the partition\u2019s metadata. This means fewer partitions are read from the storage layer or are involved in filtering and joining, which gives you better performance.</p> <p>Data in Snowflake tables will be organized based on how the data is ingested. For example, if the data in a table has been organized based on a particular column, knowing which column that is and including it in joins or in WHERE clause predicates will result in more partitions being pruned and, thus, better query performance.</p>"},{"location":"cloud-data-warehouses/snowflake/#writing-efficient-queries-best-practices","title":"Writing efficient queries: best practices <p>With the above understanding of how cloud data warehouses store and process data, these are our general recommendations for how to construct efficient queries, which is especially useful when you are querying large tables with lots of data stored in them:</p> <ol> <li>Only SELECT the columns you need. Columnar storage allows you to ignore the columns you don't need, and avoid the cost of reading it in. SELECT * can get expensive!</li> <li>If the table has a natural ordering, consider setting a clustering key. For example, if the data in the table consists of events with an associated timestamp, you might want to cluster according to that timestamp. Then events with similar times would be stored near each other in the same or adjacent partitions, and queries selecting for a particular date range would have to scan fewer partitions.</li> <li>If the table has a clustering key already set, try to filter based on that in your queries. This can greatly reduce the amount of data you need to scan. The queries based on these filters should be as simple as you can manage, complex predicates on clustered columns can make it difficult for query optimizers to prune partitions. You can tell if Snowflake has a clustering key set by inspecting the table definition for a cluster by clause.</li> <li>Filter early in complex queries, rather than at the end. If you have complex, multi-stage queries, filtering down to the subset of interest at the outset can avoid the need to process unnecessary data and then throw it away later in the query.</li> </ol>","text":""},{"location":"cloud-data-warehouses/snowflake/#exercise-2","title":"Exercise: 2","text":""},{"location":"cloud-data-warehouses/snowflake/#run-a-query-and-view-its-performance","title":"Run a query and view its performance","text":"<ol> <li>In Snowflake, make sure your role is set to TRANSFORMER_DEV</li> <li>Navigate to PROJECTS &gt; WORKSHEETS</li> <li>Create a new worksheet and ensure you have the following context set:     Database: RAW_DEV     Schema: WATER_QUALITY     Warehouse: Smallest</li> <li> <p>Copy the following code into this cell and press run</p> <pre><code>SELECT * FROM stations WHERE county_name = 'Yolo';\n</code></pre> </li> <li> <p>View the Query Details to see how long this took and other details about this query</p> </li> <li> <p>Now copy the following</p> <pre><code>SELECT * FROM lab_results where \"county_name\" = 'Yolo';\n</code></pre> </li> <li> <p>View the Query Details and compare them to that of the first query (hint: you can go back to the results of the first query by opening the Query History pane)</p> </li> <li>Next, navigate to MONITORING &gt; QUERY HISTORY and compare the two queries you ran.     -Click into each query     -Look at Query Profile to see the most expensive nodes</li> </ol>"},{"location":"code/ci-cd/","title":"Continuous Integration/Continuous Deployment","text":"<p>Continuous Integration and Continuous Deployment (or Delivery), usually abbreviated as CI/CD, are a critical part of modern software, data, and analytics engineering. Broadly speaking, they are about automating parts of the code development process:</p> <ul> <li>Continuous Integration (CI) refers to automating testing, linters, formatters, and other code quality checks.</li> <li>Continuous Deployment (CD) refers to automating build, deployment, or publishing processes.     This could happen on every merge to <code>main</code>, or could be triggered by specific actions in special repository branches.</li> </ul> <p>Automating things using CI/CD provides major benefits to development teams:</p> <ol> <li>Catch bugs and other errors before they are deployed</li> <li>Develop new features with speed and confidence</li> <li>Establish a house style with linters and formatters</li> <li>Remove humans from complex and error-prone deployment processes</li> </ol>"},{"location":"code/ci-cd/#continuous-integration-ci","title":"Continuous Integration (CI)","text":"<p>Most developers interact with CI checks on a daily basis. Usually CI checks run on every proposed change to the code base (i.e., pull requests to <code>main</code>). Examples of CI checks include:</p> <ul> <li>Code quality checks: These look for issues with your code, such as syntax errors, potential security vulnerabilities, and performance issues.</li> <li>Build checks: These ensure your code can be successfully built.</li> <li>Test checks: These run your unit tests and integration tests to ensure that they pass.</li> <li>Deployment checks: These ensure your code can be successfully deployed to a test environment.</li> </ul> <p>ODI's MDSA projects usually use a combination of dbt Cloud (setup instructions here) and GitHub Actions (setup instructions here) for running CI. We include the following CI checks:</p>"},{"location":"code/ci-cd/#pre-commit","title":"pre-commit","text":"<p>The pre-commit check is a collection of smaller checks. They are intended to be fast and cheap to run, so the entire suite runs in a couple of seconds or less.</p> <p>ODI's MDSA projects typically contains (at least) the following pre-commit checks:</p> <ul> <li>Verifying that <code>.json</code> files are valid</li> <li>Verifying that <code>.yaml</code> files are valid</li> <li>Trimming whitespace from the end of lines and files</li> <li>Checking that merge conflicts aren't committed</li> <li>Type checking, linting, and formatting Python files using <code>ruff</code> and <code>mypy</code>     (for projects that include Python)</li> <li>Linting and formatting SQL files using <code>sqlfluff</code></li> <li>Linting and formatting YAML files using <code>prettier</code> and <code>yamllint</code>.</li> </ul> <p>These checks run in two different contexts:</p> <ol> <li>They are run for the whole project on every pull request to <code>main</code> in CI.</li> <li>Since they are fast and cheap, they can be installed as git     pre-commit hooks (hence the name of the check).     For instructions on how to install pre-commit hooks locally see     these docs.</li> </ol>"},{"location":"code/ci-cd/#dbt-build","title":"dbt build","text":"<p>The dbt build check builds the dbt project in the development environment. Depending on the scale of the data, this build might take a significant amount of time, and may cost some money in Snowflake compute (as such it isn't appropriate for a pre-commit check). Building a dbt project includes the following components:</p> <ul> <li>Verifies that models can build</li> <li>Verifies that data tests pass</li> <li>Verifies that seed load</li> </ul> <p>The ODI team currently uses two different approaches to running this CI check:</p> <ol> <li>Run using dbt Cloud's continuous integration hooks</li> <li>Run using GitHub Aactions or Bitbucket Pipelines (example here)</li> </ol> <p>In both cases, a service account is used to connect to Snowflake when running the check. The schemas for models built during CI are given a prefix to indicate the source of the build, such as <code>DBT_CLOUD_PR_</code> or <code>GH_CI_PR_</code>, or <code>BITBUCKET_CI_</code>.</p>"},{"location":"code/ci-cd/#docs-build","title":"docs build","text":"<p>The docs build check ensures that the project docs build without issue (example here).</p>"},{"location":"code/ci-cd/#continuous-deployment-cd","title":"Continuous Deployment (CD)","text":"<p>Continuous Deployment (CD) in most MDSA projects is usually pretty simple. We typically do not build any applications or deploy cloud resources. Instead, whatever is in the <code>main</code> branch is considered \"production\", and our dbt projects and docs are built using that.</p> <p>ODI's MDSA projects usually include the following CD processes:</p>"},{"location":"code/ci-cd/#dbt-build_1","title":"dbt build","text":"<p>The dbt build step checks out the <code>main</code> branch of the project repository and builds the dbt project in the production environment.</p> <p>The ODI team currently uses two different approaches to running this process:</p> <ol> <li>Run using a dbt Cloud production environment</li> <li>Run using CI systems like GitHub Actions or Bitbucket Pipelines (example here)</li> </ol>"},{"location":"code/ci-cd/#docs-build_1","title":"docs build","text":"<p>The docs build step checks out the <code>main</code> branch of the project repository, builds the HTML project docs, and pushes them to GitHub Pages (example here).</p> <p>In most cases our docs CI check is the same as our docs CD process, the CI check just skips the push to GitHub pages.</p>"},{"location":"code/devops/","title":"Azure DevOps","text":""},{"location":"code/devops/#what-is-azure-devops","title":"What is Azure DevOps?","text":"<p>Azure DevOps is a suite of developer tool and services provided by Microsoft. Its intention is to streamline the software development lifecycle, enabling teams to collaborate effectively, automate processes, and deliver high-quality products at a faster pace.</p> <p>Three key features and services that the ODI CalData team use in their MDSA service are:</p> <ul> <li>Azure Pipelines</li> <li>Azure Repos (what we'll focus most of this training on!)</li> <li>Azure Boards (we'll spend a little time here as well!)</li> </ul>"},{"location":"code/devops/#what-are-azure-pipelines-repos-and-boards","title":"What are Azure Pipelines, Repos, and Boards?","text":""},{"location":"code/devops/#azure-pipelines","title":"Azure Pipelines","text":"<p>A continuous integration and continuous deployment (CI/CD) platform that automates your builds and deployments.</p>"},{"location":"code/devops/#azure-repos","title":"Azure Repos","text":"<p>Azure Repos is Microsoft's cloud service built on top of git. It provides a centralized place to store your git repositories and collaborate with your team. Here's what it offers:</p> <ul> <li>All the benefits of git: Such as branching, merging, and history tracking.</li> <li>Seamless Integration: Azure Repos integrates with other Azure DevOps services, making it easy to manage your entire code development lifecycle in one place.</li> <li>Enhanced Security: Azure DevOps provides robust access control and security features to protect your code.</li> <li>Flexibility: You can use your favorite git client (like GitHub Desktop, your command line, or VS Code) to work with your code.</li> </ul> <p>In essence, Azure Repos takes the power and flexibility of git and adds features and integrations within the Azure ecosystem, making it a comprehensive solution for managing your source code.</p>"},{"location":"code/devops/#cloning-the-azure-project-repo-locally","title":"Cloning the Azure project repo locally","text":"<p>Before you clone your repository you'll need a to do either of the following options:</p> <p>Option 1:</p> <ul> <li>Install a code editor of your choice (if you do not already have one), we highly recommend VS Code, you can read through our team's example VS Code set up for inspiration.</li> <li>Install Git Credential Manager (GCM), which makes the task of authenticating to Azure DevOps repositories easy and secure. (recommended method)<ul> <li>Mac OS: You can install GCM (with Homebrew)<ul> <li>After you install GCM run the following command: <code>brew install git-credential-manager</code></li> </ul> </li> <li>Windows: Install git for Windows which already comes bundled with GCM</li> </ul> </li> </ul> <p>Option 2:</p> <ul> <li>Install Github Desktop which also provides support for working with Azure Repos</li> <li>This option may need an installation of GCM though we haven't tested it. If so, follow the steps above depending on your machine.</li> </ul>"},{"location":"code/devops/#steps-to-clone-the-repo","title":"Steps to clone the repo","text":"<ol> <li>Go to your command line</li> <li>Type <code>cd ~</code>.</li> <li>Type git clone <code>&lt;Azure DevOps repo URL&gt;</code>.</li> </ol>"},{"location":"code/devops/#working-with-azure-repos-remotely-or-locally","title":"Working with Azure Repos remotely or locally","text":""},{"location":"code/devops/#code-branches","title":"Code branches","text":"<p>When you are writing code you often create something called a branch. This allows you to track your work separately from the main branch of the code repository. Below are the steps for how to do this in Azure Repos or locally. Furthermore, all of the subsections underneath \"Code branches\" refer to working with git via your command line interface (CLI), we recommend bookmarking this GitHub git cheat sheet for easy reference in addition to what we've provided.</p> <p>Creating or switching branches in Azure Repos:</p> <ol> <li>Go to your Azure DevOps repository.</li> <li>Click on the \"Branches\" tab.</li> <li>Click \"New branch\".</li> <li>Enter a descriptive name for your branch (e.g., \"feature/new-feature\").</li> <li>Select the base branch (usually \"main\").</li> <li>Click \"Create\".</li> </ol> <p>Creating or switching branches locally:</p> <p>If you're working with git locally, you can create and switch branches as well as check your current git branch using the following git commands:</p> <ul> <li>Create and switch to a branch: <code>git switch -c &lt;branch_name&gt;</code><ul> <li>This also works, but is the older way of doing it: <code>git checkout -b &lt;branch_name&gt;</code></li> </ul> </li> <li>Switch to an existing branch: <code>git switch &lt;branch_name&gt;</code><ul> <li>If this doesn\u2019t work it\u2019s likely that you created a branch remotely. You have to pull down (or fetch) the remote branch you want to work with using <code>git fetch</code></li> <li>Then you can run <code>git switch &lt;branch_name&gt;</code> again</li> </ul> </li> <li>Check which branch you are on: <code>git branch --show-current</code></li> </ul> <p>Once you are satisfied with the work you've done on your branch you will want to save (stage) it and commit it. Below are the steps for how to do this locally.</p> <p>Staging and committing your changes:</p> <ol> <li>Create or switch your branch<ol> <li>Make sure you\u2019re on the branch you want to commit changes to (you almost never want to commit directly to \u201cmain\u201d). If you just created a branch you are likely already on the branch you want to make changes to, if not switch to it.</li> <li>To create or switch branches, follow the steps above</li> </ol> </li> <li>Navigate to the file you want to modify and make the necessary changes<ol> <li>Whether you\u2019re editing a file in dbt Cloud, locally with a code editor like VS Code, or directly in Azure (not recommended) you must click SAVE or use CTRL/COMMAND S. To save the changes to your file BEFORE you commit them.</li> </ol> </li> <li>Stage your changes<ol> <li>We\u2019ll skip how to do this in Azure Repos because we don\u2019t recommend editing and committing changes directly in Azure Repos as this doesn\u2019t allow you to run linting tools that can help you catch errors BEFORE you push changes. We will have CI checks set up on the project repo that will help catch database, formatting, SQL errors, etc.</li> <li>Locally this is done by:<ol> <li>Tying <code>git add &lt;file_name.file_extension&gt;</code></li> <li>You can use <code>git add .</code> to add ALL files you\u2019ve edited. This can be a dangerous operation because you may accidentally stage files you made changes to but did not want to be added to the project repo. For instance you could have made changes to a file that may contain sensitive information. Only use <code>git add .</code> if you are sure all files are safe to stage!</li> </ol> </li> <li>In dbt Cloud the git add process is handled under the hood so be sure that every file you edit is actually a file you want to later commit, if not you must revert changes to any files you do not want to commit.</li> </ol> </li> <li>Commit your changes<ol> <li>Again we\u2019ll skip how to do this in Azure \u2013 we do not recommend it!</li> <li>Locally this is done with: <code>git commit -m \u201c&lt;a short message about the changes you made&gt;\u201d</code></li> <li>In dbt Cloud this is done by:<ol> <li>Clicking the \u201ccommit and sync\u201d button</li> <li>Then typing a short, yet descriptive message about the changes you made in the text box that appears</li> <li>Then clicking \u201cCommit Changes\u201d </li> </ol> </li> </ol> </li> </ol> <p>Pushing your changes:</p> <ol> <li>Locally this is done with: <code>git push origin &lt;branch_name&gt;</code></li> <li>In dbt Cloud this is also done under the hood when you click \u201cCommit Changes\u201d</li> </ol>"},{"location":"code/devops/#pull-requests","title":"Pull requests","text":"<p>To submit the changes you've pushed to the codebase you'll open a pull request (PR). A PR proposes changes to a code repository. It's a formal request to merge your changes into the main branch of the repository.</p> <p>Benefits of Using PRs:</p> <ul> <li>Collaboration: PRs allow developers to collaborate on code and share ideas.</li> <li>Code review: PRs allow for feedback on your changes. You can read about how the ODI CalData team approaches code review here</li> <li>Testing: PRs can test changes before merging.</li> <li>Documentation: PRs can document changes to the code.</li> <li>History: PRs provide a history of changes.</li> </ul> <p>Opening a PR:</p> <p>in Azure Repos:</p> <p>The official Microsoft documentation related to Azure Repos PRs is here (ignore cherry-pick section). Summarized steps are listed below.</p> <ol> <li>Go to your Azure DevOps repository.</li> <li>Click on the \"Pull requests\" tab.</li> <li>Click \"New pull request\".</li> <li>Select the source branch (your feature branch) and the target branch (usually \"main\").</li> <li>Add a descriptive title and detailed description.</li> <li>Add yourself as a reviewer and select other reviewers as needed.</li> <li>Click \"Create\"</li> </ol> <p>in dbt Cloud:</p> <ol> <li>After you commit your changes you\u2019ll see a light green button on the upper left that says \u201cCreate a pull request...\u201d. This will only appear if you\u2019ve yet to open a PR. If you have already opened a PR and are simply committing more changes to it you will not see this option.</li> </ol> <p>Reviewing a PR:</p> <ol> <li>Go to the \"Pull requests\" tab in your Azure DevOps repository.</li> <li>Find the PR you want to review.</li> <li>Review the changes, leave comments, and suggest changes if necessary.</li> <li>Approve the PR or request changes.</li> </ol> <p>Suggesting changes to a PR:</p> <ul> <li>When reviewing a PR, you can suggest changes directly in the code using the Azure DevOps interface.</li> <li>Alternatively you can open your own branch and suggest changes this way</li> </ul> <p>Resolving a merge conflict:</p> <ul> <li>Merge conflicts occur when changes are made to the same part of a file in both the source and target branches.</li> <li>Resolve merge conflicts manually by editing the file and choosing which changes to keep.</li> <li>Stage, commit, and push the resolved changes.</li> </ul> <p>Merging a PR:</p> <ul> <li>As a reviewer, you can merge a PR after you approve it and all CI checks pass.</li> <li>Click the \"Complete\" button on the PR page to merge the changes.</li> </ul>"},{"location":"code/devops/#azure-boards","title":"Azure Boards","text":"<p>A work tracking system that supports agile methodologies. Teams can create backlogs, plan sprints, track work items, and visualize progress.</p>"},{"location":"code/devops/#azure-boards-work-items","title":"Azure Boards work items","text":"<p>In Azure Boards, work items are how we document and track our work, think of is as a task. A well-written work item provides clear information on the what, why, and who, enabling efficient collaboration and quicker problem resolution. They serve as documentation, capturing decisions and discussions valuable for current and future team members. Clear work items also help with task prioritization and progress tracking.</p> <p>When writing a work item you can use markdown, this Microsoft reference document is a great starting point.</p> <p>Creating a work item:</p> <ol> <li>Go to the Boards hub in your Azure DevOps project.</li> <li>Choose the appropriate Work Item Type (e.g., User Story, Task, Bug).</li> <li>Fill in the work item form. At a minimum, include:</li> <li>Title: A clear, concise, and descriptive title.</li> <li>Description: A detailed description of the work.</li> <li>Assigned To: The team member responsible for the work.</li> <li>Area Path: The area of the project this work belongs to.</li> <li>Iteration Path: The sprint or iteration this work is scheduled for.</li> <li>State: (e.g., To Do, In Progress, Done).</li> <li>Priority: (e.g., Urgent, High, Medium, Low)</li> <li>Effort: (e.g., Story Points, hours)</li> </ol> <p>Tips for effective work items:</p> <ul> <li>Use clear and concise language.</li> <li>Provide all relevant details.</li> <li>Attach relevant files or screenshots.</li> <li>Link related work items.</li> <li>Use tags to categorize work items.</li> </ul> <p>Work items magic:</p> <ul> <li>Linking to Code: You can link work items to commits, pull requests, and branches. This is great for traceability as it helps track the progress of work and provides context for code changes.</li> <li>@Mentioning Teammates: Type \"@\" followed by your teammate's name to notify them in comments and descriptions.</li> </ul> <p>Azure DevOps Documentation:</p> <ul> <li>Azure Repos | Microsoft Learn</li> <li>Azure DevOps | Microsoft Learn</li> </ul>"},{"location":"code/git/","title":"git","text":""},{"location":"code/git/#what-is-git","title":"What is git?","text":"<p>Git, a distributed version control system, is software that you install locally on your machine that enables source code management. Code is organized into folder-like structures called git repositories, which enables you to track the history of changes to the code, safely develop features in side branches, and collaborate with others. Git is a distributed version control system, which means that each developer has a local copy of the entire code repository. This makes it easy to work on code offline and to share changes with other developers.</p> <p>Git is an essential tool for software development as it allows developers to track changes to code, collaborate, and share open or closed-source code.</p>"},{"location":"code/git/#learning-git","title":"Learning git","text":"<p>There are many high-quality resources for learning git online. Here are a few:</p> <ul> <li>Atlassian has an excellent set of tutorials for learning git, including:</li> <li>A conceptual overview for beginners</li> <li>How to set up a repository</li> <li>How to use git to collaborate with others</li> <li>GitHub has a nice cheat-sheet for common git commands</li> <li>The official git documentation is not always the most user-friendly, but it has a depth of information that isn't available elsewhere</li> </ul> <p>If you have a remote repository set up on something like GitHub or Azure DevOps and want to make a local copy of it you have to run the <code>git clone</code> command. Once you have a working copy of your remote repo, all version control operations are managed through this local copy. Follow the steps below to clone a repository.</p>"},{"location":"code/git/#clone-a-repo-locally","title":"Clone a repo locally","text":"<p>for MacOS or Linux-based CLI</p> <ol> <li>Go to your command line</li> <li>Type <code>cd ~</code></li> <li>Type <code>git clone &lt;https://github.com/org/repo-name.git&gt;</code></li> </ol>"},{"location":"code/git/#other-ways-to-work-with-git","title":"Other ways to work with git","text":"<ul> <li>Install Git for Windows</li> <li>Or install GitHub for Desktop</li> </ul>"},{"location":"code/github/","title":"GitHub","text":""},{"location":"code/github/#what-is-github","title":"What is Github?","text":"<p>GitHub is a web-based hosting service for git repositories. It provides a graphical user interface (GUI) for managing and reviewing code, as well as social networking features for interacting with other developers.</p>"},{"location":"code/github/#clone-a-repo-locally","title":"Clone a repo locally","text":"<p>with GitHub Desktop</p> <ol> <li>Follow these instructions to download GitHub desktop if you do not already have it.</li> <li>Be sure to login with the GitHub username you are using for the repo you wish to clone</li> <li>Be sure to select the correct repository to clone e.g. org/repo-name</li> </ol>"},{"location":"code/github/#pull-requests","title":"Pull Requests","text":"<p>A pull request (PR) is a way to propose changes to a code repository. It is a formal request to the repository owner(s) to merge your changes into their codebase, often the \u201cmain\u201d branch.</p> <p>PRs are typically created by cloning or forking the repository, making changes to the code, and then submitting a request to the repository owner to merge your changes. The repository owner can then review your changes, request changes \u2013 if necessary, and decide whether to merge them into their codebase.</p> <p>Here are some of the benefits of using PRs:</p> <ul> <li>Collaboration: PRs allow developers to collaborate on code and share ideas.</li> <li>Code review: PRs allow other developers to review your changes and provide feedback.</li> <li>Testing: PRs can be used to test changes before they are merged into the main codebase.</li> <li>Documentation: PRs can be used to document changes to the code.</li> <li>History: PRs provide a history of changes to the code.</li> </ul>"},{"location":"code/github/#creating-or-switching-branches","title":"Creating or switching branches","text":""},{"location":"code/github/#github-branch-management","title":"GitHub Branch management","text":"<ol> <li>Go to your Repository \u27a1\ufe0f <code>&lt;https://github.com/org/repo-name&gt;</code></li> <li>Click on the Branch Dropdown<ol> <li>On the main page of your repository, you'll see a dropdown menu on the top left, displaying the current branch (usually \u201cmain\u201d). Click on this dropdown.</li> </ol> </li> <li>Type or click on a Branch Name<ol> <li>In the dropdown that appears, there\u2019s a textbox at the top, type the name for the branch you want to switch to or create.</li> <li>When creating a new branch, choose a descriptive and relevant name. e.g. <code>water_quality_staging_model</code></li> <li>If you\u2019re just switching branches you are done after this step!</li> </ol> </li> <li>Choose a base for the New Branch<ol> <li>Usually this is \u201cmain\u201d as you want to work off of the most up-to-date version of code, but you can also choose another branch to base your new one off of if you want to add to someone else\u2019s work in progress without making edits directly to their branch and causing a merge conflict! \ud83d\ude2c</li> </ol> </li> <li>Click \"Create branch &lt;branch_name&gt; from &lt;base_branch_name&gt;\" (usually your base branch will be \u201cmain\u201d)</li> </ol> <p>Now, you have successfully created a new branch in your GitHub repository and will be automatically switched to this branch. You can make changes on this new branch without affecting the main branch.</p>"},{"location":"code/github/#local-branch-management","title":"Local Branch management","text":"<p>If you're working with git locally, you can create and switch branches as well as check your current git branch using the following git commands:</p> <ul> <li>Create and switch to a branch: <code>git switch -c &lt;branch_name&gt;</code><ul> <li>This also works, but is the older way of doing it: <code>git checkout -b &lt;branch_name&gt;</code></li> </ul> </li> <li>Switch to an existing branch: <code>git switch &lt;branch_name&gt;</code><ul> <li>If this doesn\u2019t work it\u2019s likely that you created a branch remotely. You have to pull down (or fetch) the remote branch you want to work with using <code>git fetch</code></li> <li>Then you can run <code>git switch &lt;branch_name&gt;</code> again</li> </ul> </li> <li>Check which branch you are on: <code>git branch --show-current</code></li> </ul> <p>Note</p> <p>Replace \u201c<code>&lt;branch_name&gt;</code>\u201d with the actual name of your new branch e.g. <code>writing_water_quality_docs</code>.</p> <p>This is optional, but it may be helpful to you to persistently show your full path and branch name in your command line. We've outlined how to do it below.</p> <p>For Mac OS</p> <ul> <li>Type <code>open ~/.bash_profile</code></li> <li>In the file that opens, add the following at the bottom of the file:</li> </ul> <pre><code>    parse_git_branch() {\n    \u00a0\u00a0\u00a0\u00a0git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/ (\\1)/'\n    }\n\n    export PS1=\"\\u@\\h \\[\\033[32m\\]\\w -\\$(parse_git_branch)\\[\\033[00m\\] $ \"\n</code></pre> <p>For Windows OS</p> <ul> <li>If you do not have git bash installed please download it here</li> <li>Showing branch names is a built-in feature of git bash. If branches do not show in your command prompt, follow these steps:<ul> <li>Download the official git-prompt script. Move it to somewhere accessible to git bash (e.g. <code>~/.git-prompt.sh</code>)</li> <li>Add the following 2 lines to your <code>~/.bashrc</code> file:</li> </ul> </li> </ul> <pre><code>    source ~/.git-prompt.sh\n\n    export PS1='\\[\\033[32m\\]\\u@\\h \\[\\033[35m\\]\\w\\[\\033[36m\\]$(__git_ps1 \" (%s)\")\\[\\033[0m\\]\\$ '\n</code></pre> <p>Follow these remaining steps regardless of OS</p> <ul> <li>Save your file and close it</li> <li>Open a new terminal window</li> <li>Type <code>cd ~/caldata-mdsa-&lt;project-name&gt;</code></li> <li>You\u2019ll now see something like this: </li> </ul>"},{"location":"code/github/#staging-and-committing-changes","title":"Staging and committing changes","text":"<ol> <li>Create or switch your branch<ol> <li>Make sure you\u2019re on the branch you want to commit changes to (you almost never want to commit directly to \u201cmain\u201d). If you just created a branch you are likely already on the branch you want to make changes to, if not switch to it.</li> <li>To create or switch branches, follow the steps above</li> </ol> </li> <li>Navigate to the file you want to modify and make the necessary changes<ol> <li>Whether you\u2019re editing a file in dbt Cloud, locally with a code editor like VS Code, or directly in GitHub (not recommended) you must click SAVE or use CTRL/COMMAND S. To save the changes to your file BEFORE you commit them.</li> </ol> </li> <li>Stage your changes<ol> <li>We\u2019ll skip how to do this in GitHub because we don\u2019t recommend editing and committing changes directly in GitHub as this doesn\u2019t allow you to run linting tools that can help you catch errors BEFORE you push changes. We will have CI checks set up on the project repo that will help catch database, formatting, SQL errors, etc.</li> <li>Locally this is done by:<ol> <li>Tying <code>git add &lt;file_name.file_extension&gt;</code></li> <li>You can use <code>git add .</code> to add ALL files you\u2019ve edited. This can be a dangerous operation because you may accidentally stage files you made changes to but did not want to be added to the project repo. For instance you could have made changes to a file that may contain sensitive information. Only use <code>git add .</code> if you are sure all files are safe to stage!</li> </ol> </li> <li>In dbt Cloud the git add process is handled under the hood so be sure that every file you edit is actually a file you want to later commit, if not you must revert changes to any files you do not want to commit.</li> </ol> </li> <li>Commit your changes<ol> <li>Again we\u2019ll skip how to do this in GitHub \u2013 we do not recommend it!</li> <li>Locally this is done with: <code>git commit -m \u201c&lt;a short message about the changes you made&gt;\u201d</code></li> <li>In dbt Cloud this is done by:<ol> <li>Clicking the \u201ccommit and sync\u201d button</li> <li>Then typing a short, yet descriptive message about the changes you made in the text box that appears</li> <li>Then clicking \u201cCommit Changes\u201d </li> </ol> </li> </ol> </li> </ol>"},{"location":"code/github/#pushing-your-changes","title":"Pushing your changes","text":"<ol> <li>Locally this is done with: <code>git push origin &lt;branch_name&gt;</code></li> <li>In dbt Cloud this is also done under the hood when you click \u201cCommit Changes\u201d</li> </ol>"},{"location":"code/github/#opening-a-pr","title":"Opening a PR","text":"<p>Option 1: This works whether you commit changes locally or via dbt Cloud</p> <ol> <li>Go to the GitHub repository where you just pushed your changes</li> <li> <p>At the top of the home page you\u2019ll see a message like the one below. It\u2019ll say \u201c&lt;your_branch_name&gt; had recent pushes X minutes ago\u201d with a green button that says \u201cCompare &amp; pull request\u201d. Click that button.</p> <p></p> </li> <li> <p>Next you\u2019ll be taken to a new screen like the one shown below. </p> </li> <li>From here you\u2019ll:<ol> <li>Check that your branch is \u201cAble to merge\u201d (as seen in the upper center of the screen with a preceding green checkmark)<ol> <li>If you see \u201cCan\u2019t automatically merge.\u201d that means you have a merge conflict. We cover how to resolve merge conflicts below.</li> </ol> </li> <li>Add a more descriptive title and detailed description</li> <li>Add yourself as an Assignee</li> <li>Select a teammate as a Reviewer</li> <li>You\u2019ll have options to fill in other details like projects, we\u2019ll cover those later</li> </ol> </li> <li>Click the green button on the lower right that says \u201cCreate pull request\u201d</li> </ol> <p>Note</p> <p>This option only works for an hour after you have pushed your changes. If you don\u2019t open a pull request within that 60 minute window this button will disappear. Fear not! There is a second way to open a pull request outlined below.</p> <p>Option 2: This is the option to use if you cannot follow step 2 in Option 1.</p> <ol> <li>Go to the Pull Requests page on GitHub directly (example) or go to your repo's homepage and click on the \u201cPull requests\u201d tab near the top as pictured </li> <li>Click the green \u201cNew pull request\u201d button in the upper right corner</li> <li>You\u2019ll be taken to a new window</li> <li>Click the button that says \u201ccompare: main\u201d</li> <li>A dropdown will open, from there you can either type or click the name of the branch you want to compare to the \u201cbase: main\u201d branch.</li> <li>After you select the branch follow steps 3 through 5 from Option 1 above</li> </ol> <p>Option 3: You have a third option to open a PR in dbt Cloud if you don\u2019t choose to follow either of the two options above.</p> <ol> <li>After you commit your changes you\u2019ll see a light green button on the upper left that says \u201cCreate a pull request on GitHub\u201d. This will only appear if you\u2019ve yet to open a PR. If you have already opened a PR and are simply committing more changes to it you will not see this option. </li> </ol>"},{"location":"code/github/#reviewing-a-pr","title":"Reviewing a PR","text":"<p>The ODI CalData team put together documentation on reviewing a PR with the two core messages being, have empathy and get CI (continuous integration) to pass.</p> <ol> <li>Navigate to the PR<ol> <li>Go to the the \u201cPull Requests\u201d tab in project repository where the PR was opened and find the PR you want to review</li> <li>You can also find PRs where you are the requested reviewer by going directly to this link: github.com/pulls/review-requested</li> </ol> </li> <li>Review the changes<ol> <li>GitHub will take you to the home screen of the PR which starts on the \u201cConversation\u201d tab. This is where you can read any commits by the PR author, anyone involved in review, and any automated tools.\u00a0</li> <li>The \u201cCommits\u201d tab is where you can check each save to the PR to understand the sequence of changes</li> <li>The \u201cChecks\u201d tab is where you can see the jobs run by GitHub actions (CI automations). You can see whether or not they pass or fail and the details of each.</li> <li>There will be a yellow pane across the top of this page like you see pictured </li> <li>Clicking the green \u201cAdd your review\u201d button will take you to the \u201cFiles changed\u201d tab where you can begin your review.<ol> <li>In the files changed tab you can leave a comment on any line of code by clicking the blue plus sign that appears when you hover. You can leave a single comment that is not part of a review or leave one or many comments as part of your review. You can suggest changes here too which we\u2019ll cover in the next section.</li> </ol> </li> <li>After you\u2019re done with your review, if you scroll back to the top there will be a green button on the upper right that says \u201cFinish your review\u201d. Click that and decide if you just want to do one of the following: 1) Comment, 2) Approve or 3) Request changes. Then click the green button on the lower right that says \u201cSubmit review\u201d.</li> </ol> </li> </ol>"},{"location":"code/github/#suggesting-changes-to-a-pr","title":"Suggesting changes to a PR","text":"<p>When you\u2019re reviewing a PR instead of just commenting on a line of code you may want to suggest changes directly to the code. You can do this by clicking the blue plus sign button next to the line of code you want to suggest changes to.</p> <p>In the window that opens click the button that has a + and - sign as pictured below. </p> <p>In this example, \u201ctest\u201d is misspelled so the PR reviewer is adding a suggestion and fixing the code with the correct spelling. If the PR author agrees they can seamlessly accept this suggestion and integrate it into their code.</p> <p>After adding your suggestion and additional comments, if applicable, click the green \u201cStart a review\u201d button.</p>"},{"location":"code/github/#resolving-a-merge-conflict","title":"Resolving a merge conflict","text":"<p>Merge conflicts occur in git when the software is unable to automatically merge changes from two different branches. This happens when changes are made to the same part of a file in both the source and target branches (these are often interchanged as feature and destination branches, respectively) and git is unable to determine which of the changes should be kept.</p> <p>Below we\u2019ll step through a more detailed explanation of how a merge conflict happens and how to resolve it:</p> <ol> <li>Parallel changes occur<ol> <li>Let\u2019s say you\u2019re working on a branch called <code>feature_branch</code> where you\u2019re working on a file called docs.md. Meanwhile, a teammate makes changes to that same file in another branch <code>feature_branch_2</code> that gets merged to the main branch before yours does.</li> </ol> </li> <li>Git attempts a merge<ol> <li>When you try to merge feature_branch into the main branch, git will attempt to merge the changes. If the changes are in different parts of the same file, git can usually merge them seamlessly.</li> </ol> </li> <li>Conflict arises<ol> <li>If changes are made to the same part of the same file in both branches, git will detect a conflict. It is unable to reconcile the differences in files as it doesn\u2019t know which changes to keep and which to discard.</li> </ol> </li> <li> <p>Conflict markers</p> <ol> <li>If attempting to merge from your command line you\u2019ll see the following</li> </ol> <p></p> <ol> <li>Git will also mark the conflicting sections of the file with special markers. The conflicting changes from both branches are placed between these markers. Like you see below</li> </ol> <p></p> <ol> <li>If you navigate to GitHub to create a pull request you\u2019ll see the following</li> </ol> <p></p> </li> <li> <p>Resolution</p> </li> </ol> <p>Manually</p> <ol> <li>To resolve the conflict you need to manually edit the file to choose what changes to keep. Remove the conflict markers and any code that is not needed.</li> <li>After you resolve the conflict you\u2019ll need to stage the file again with <code>git add &lt;file_name.file_extension&gt;</code></li> <li>You\u2019ll need to commit the changes again with <code>git commit - m \u201c&lt;a short message about the changes you made&gt;\u201d</code></li> <li>Type <code>git merge --continue</code></li> <li>Finally, type <code>git push origin main</code></li> </ol> <p>In VS Code</p> <ol> <li> <p>You\u2019ll see the following along with a UI to actually help you decide which changes to keep</p> <p></p> </li> <li> <p>In the lower right corner of your screen you\u2019ll see a blue button that says \u201cResolve in Merge Editor\u201d. Click this button.</p> </li> <li> <p>Next you\u2019ll be taken to a screen like you see below. On this screen you\u2019ll have options on either side of your Incoming and Current changes to select the following options: Accept Incoming | Accept Combination | Ignore</p> </li> <li> <p>Select the appropriate option, this may require discussion with your team.</p> </li> <li> <p>After you decide which changes to keep, click the blue \u201cComplete Merge\u201d button in the lower right corner of\u00a0 your screen </p> </li> </ol> <p>To avoid or reduce the occurrence of merge conflicts, it\u2019s a good practice to regularly pull changes from the main branch into your feature branch. Open communication with your team about changes will also help prevent conflicts.</p>"},{"location":"code/github/#merging-a-pr","title":"Merging a PR","text":"<p>As a reviewer you have the responsibility to merge a PR after you approve it. PRs should not be merged if any of the following are true:</p> <ul> <li>All CI checks have not passed (2 out of 3 passing isn\u2019t good enough)</li> <li>You haven\u2019t approved the PR</li> <li>Requested changes have not be addressed or planned for in another PR</li> <li>The \u2018Merge pull request\u201d button is green</li> </ul> <p>If no changes are needed or if requested changes are made and all CI checks have passed, you can merge the PR by clicking the \u201cMerge pull request\u2019 button on the \u201cConversation\u201d tab of the PR.</p> <p>After you click this,\u00a0 GitHub will prompt you to choose which email to associate with the commit and then to click the \u201cConfirm merge\u201d button.</p> <p>At this point, GitHub will finalize the merge process and these changes will now be incorporated into the main branch. You may delete the feature branch that was used to submit the PR.</p>"},{"location":"code/github/#github-issues","title":"GitHub Issues","text":"<p>Issues on GitHub are how we document and track our work. Writing a good issue is important because it provides clear information on the what, why, and who relating to a body of work which can enable efficient collaboration. Well documented issues allow for quicker understanding and problem resolution. They serve as a form of documentation capturing decisions and discussion which is valuable to reference for current and future team members. Clear issues also help with task prioritization and milestone tracking across our work.</p>"},{"location":"code/github/#creating-an-issue","title":"Creating an issue","text":"<ol> <li>Go to the \u201cIssues\u201d tab of the project repository</li> <li>Find and click on the green \u201cNew issue\u201d button in the upper right corner</li> <li>At a minimum your issue should have 5 main elements and 5 more sub-elements.<ol> <li>A clear, concise, and descriptive title</li> <li>A detailed description of the issue</li> <li>An assignee, usually yourself, but you may assign someone else more appropriate</li> <li>A project it corresponds to</li> </ol> </li> <li>After the issue is created click on the dropdown next to your project in the \u201cProjects\u201d section on the right side of your issue. Add the following details:<ol> <li>Status (Backlog, To do, In progress, Needs Review, Blocked, etc.)</li> <li>Priority (e.g. Urgent, High, Medium, Low)</li> <li>Size (X-Large, Large, Medium, Small, Tiny)</li> <li>Sprint (either current or future)</li> <li>Project (e.g. DIF - Caltrans)</li> </ol> </li> <li>A relevant milestone that this issue fits into</li> </ol> <p>Here is an example GitHub issue that has all 10 elements and sub-elements listed above.</p>"},{"location":"code/github/#writing-markdown","title":"Writing Markdown","text":"<p>Writing markdown is important to learn when creating project documentation in markdown files (.md) and for writing GitHub issues. GitHub has handy documentation on basic Markdown syntax which is a great starting point.</p>"},{"location":"code/github/#github-magic","title":"GitHub Magic \u2728","text":"<ol> <li>Linking PRs to Issues and Issues to PRs<ol> <li>You can do this by going to the \u201cDevelopment\u201d section of your issue on the right hand side underneath \u201cMilestone\u201d. Click on the cog wheel then select the relevant repository the PR lives in. From there type in the number (e.g. 266) or title of the PR you want to link or select it from the dropdown that appears.\u00a0</li> <li>You can also do this by using a keyword like \u201ccloses, fixes, or resolves\u201d followed by # and the issue number, e.g. Fixes #143</li> <li>When you use the \u201ccloses\u201d keyword, this will close the corresponding issue when the PR is merged</li> <li>For a full list of keywords you can use to link a PR to an issue checkout GitHub\u2019s documentation complete with examples</li> </ol> </li> <li>Tagging a teammate<ol> <li>This is done by typing @ and selecting or typing your teammate\u2019s username in the pop-up window that appears</li> <li>You can tag teammates in your commit message, issue or PR comments, and issue descriptions</li> </ol> </li> </ol>"},{"location":"code/local-repo-setup/","title":"Local repository setup","text":"<p>Working with dbt Core will involve more than just its installation. We have included additional instructions for individual contributors to set up their local repository to work well with dbt, their data warehouse, and a git-based version control workflow with checks that can be run on the code before opening a PR.</p> <p>Here's a diagram of the steps you can expect to take:</p> <pre><code>flowchart TD\n    a[Install dependencies]\n    b[Data warehouse set up - optional]\n    c[Configure AWS - optional]\n    d[Install dbt]\n    e[Install pre-commit hooks]\n    a --&gt; b --&gt; c --&gt; d --&gt; e\n</code></pre>"},{"location":"code/local-repo-setup/#1-install-dependencies","title":"1. Install dependencies","text":"<p>Much of the software in this project is written in Python. It is usually a good idea to install Python packages into a virtual environment, which allows them to be isolated from those in other projects which might have different version constraints.</p>"},{"location":"code/local-repo-setup/#1-install-uv","title":"1. Install <code>uv</code>","text":"<p>We use <code>uv</code> to manage our Python virtual environments. If you have not yet installed it on your system, you can follow the instructions for it here. Most of the ODI team uses Homebrew to install the package. We do not recommend installing <code>uv</code> using <code>pip</code>: as a tool for managing Python environments, it makes sense for it to live outside of a particular Python distribution.</p>"},{"location":"code/local-repo-setup/#2-install-python-dependencies","title":"2. Install Python dependencies","text":"<p>If you prefix your commands with <code>uv run</code> (e.g. <code>uv run dbt build</code>), then <code>uv</code> will automatically make sure that the appropriate dependencies are installed before invoking the command.</p> <p>However, if you want to explicitly ensure that all of the dependencies are installed in the virtual environment, run <pre><code>uv sync\n</code></pre> in the root of the repository.</p> <p>Once the dependencies are installed, you can also \"activate\" the virtual environment (similar to how conda virtual environments are activated) by running <pre><code>source .venv/bin/activate\n</code></pre> from the repository root. With the environment activated, you no longer have to prefix commands with <code>uv run</code>.</p> <p>Which approach to take is largely a matter of personal preference:</p> <ul> <li>Using the <code>uv run</code> prefix is more reliable, as dependencies are always resolved before executing.</li> <li>Using <code>source .venv/bin/activate</code> involves less typing.</li> </ul>"},{"location":"code/local-repo-setup/#3-install-go-dependencies-optional","title":"3. Install go dependencies (optional)","text":"<p>This step is optional, you only need to do it if you intend to work with Terraform. ODI uses Terraform to manage infrastructure. Dependencies for Terraform (mostly in the go ecosystem) can be installed via a number of different package managers.</p> <p>If you are running Mac OS, you can install these dependencies with Homebrew.</p> <p>If you are running Windows, you can install these dependencies with Chocolatey.</p> <p>Install the go dependencies (Mac OS):</p> <pre><code>brew install terraform terraform-docs tflint go\n</code></pre> <p>Validate the install with:</p> <pre><code>terraform -v\n\ngo version\n</code></pre>"},{"location":"code/local-repo-setup/#2-data-warehouse-set-up-optional","title":"2. Data warehouse set up (optional)","text":""},{"location":"code/local-repo-setup/#1-configure-snowflake","title":"1. Configure Snowflake","text":"<p>In order to use Snowflake (as well as the terraform validators for the Snowflake configuration) you should set some default local environment variables in your environment. This will depend on your operating system and shell.</p> <p>For Windows systems, you can set persistent environment variables by navigating to Control Panel -&gt; System -&gt; Advanced system settings -&gt; Environmental variables. There, you can add the variables and values as outlined in the bash instructions below.</p> <p>For Linux and Mac OS systems, as well as users of Windows subsystem for Linux (WSL) environment variables are often set in <code>~/.zshrc</code>, <code>~/.bashrc</code>, or <code>~/.bash_profile</code>.</p> <p>If you use zsh or bash, open your shell configuration file, and add the following lines:</p>"},{"location":"code/local-repo-setup/#transformer-role-default","title":"Transformer role (default)","text":"<pre><code>export SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_DATABASE=TRANSFORM_DEV\nexport SNOWFLAKE_USER=&lt;your-username&gt;\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt;\nexport SNOWFLAKE_ROLE=TRANSFORMER_DEV\nexport SNOWFLAKE_WAREHOUSE=TRANSFORMING_XS_DEV\nexport SNOWFLAKE_AUTHENTICATOR=EXTERNALBROWSER or USERNAME_PASSWORD_MFA\n</code></pre> <p>Open a new terminal and verify that the environment variables are set.</p>"},{"location":"code/local-repo-setup/#loader-role","title":"Loader role","text":"<pre><code>export SNOWFLAKE_ACCOUNT=&lt;org_name&gt;-&lt;account_name&gt; # format is organization-account\nexport SNOWFLAKE_DATABASE=RAW_DEV\nexport SNOWFLAKE_USER=&lt;your-username&gt;\nexport SNOWFLAKE_PASSWORD=&lt;your-password&gt;\nexport SNOWFLAKE_ROLE=LOADER_DEV\nexport SNOWFLAKE_WAREHOUSE=LOADING_XS_DEV\nexport SNOWFLAKE_AUTHENTICATOR=EXTERNALBROWSER or USERNAME_PASSWORD_MFA\n</code></pre> <p>This will enable you develop scripts for loading raw data into the development environment. Again, open a new terminal and verify that the environment variables are set.</p>"},{"location":"code/local-repo-setup/#3-configure-aws-optional","title":"3. Configure AWS (optional)","text":"<p>In order to create and manage AWS resources programmatically, you need to create access keys and configure your local setup to use them:</p> <ol> <li>Install the AWS command-line interface.</li> <li>Go to the AWS IAM console and create an access key for yourself.</li> <li>In a terminal, enter <code>aws configure</code>, and add the access key ID and secret access key when prompted. We use <code>us-west-2</code> as our default region.</li> </ol>"},{"location":"code/local-repo-setup/#4-configure-dbt","title":"4. Configure dbt","text":"<p>The connection information for your data warehouses will, in general, live outside of this repository. This is because connection information is both user-specific and usually sensitive, so it should not be checked into version control.</p> <p>In order to run this project locally, you will need to provide this information in a YAML file. Run the following command to create the necessary folder and file.</p> <pre><code>mkdir ~/.dbt &amp;&amp; touch ~/.dbt/profiles.yml\n</code></pre> <p>Instructions for writing a <code>profiles.yml</code> are documented here, there are specific instructions for Snowflake here, and you can find an example below as well.</p> <p>A minimal version of a <code>profiles.yml</code> for dbt development is:</p> <pre><code>&lt;profile-name&gt;:\n  target: dev\n  outputs:\n    dev:\n      type: snowflake\n      account: &lt;account-locator&gt;\n      user: &lt;your-username&gt;\n      password: &lt;your-password&gt;\n      authenticator: externalbrowser or username_password_mfa\n      role: TRANSFORMER_DEV\n      database: TRANSFORM_DEV\n      warehouse: TRANSFORMING_XS_DEV\n      schema: DBT_&lt;first-name-inital-lastname&gt;   # e.g. DBT_JDOE\n      threads: 4\n</code></pre> <p>Note</p> <p>The target name (<code>dev</code>) in the above example can be anything. However, we treat targets named <code>prd</code> differently in generating custom dbt schema names (see here). We recommend naming your local development target <code>dev</code>, and only include a <code>prd</code> target in your profiles under rare circumstances.</p> <p>You can include profiles for several databases in the same <code>profiles.yml</code>, (as well as targets for production), allowing you to develop in several projects using the same computer.</p> <p>You can verify that your <code>profiles.yml</code> is configured properly by running the following command in the project root directory (<code>transform</code>).</p> <pre><code>uv run dbt debug\n</code></pre>"},{"location":"code/local-repo-setup/#vs-code-setup-optional","title":"VS Code setup (optional)","text":"<p>Many people prefer to use featureful editors when doing local development so we included an example set up with VS Code. By equipping a text editor like VS Code with an appropriate set of extensions and configurations we can largely replicate the dbt Cloud experience locally. Below is one possible configuration for VS Code.</p> <p>Install some useful extensions (this list is advisory, and non-exhaustive):</p> <ol> <li>dbt's official VS Code extension<ol> <li>Follow dbt's docs for instructions on installation.</li> <li>Test that the dbt VS Code extension extension is working by opening one of the project model <code>.sql</code> files and building or running the model.</li> </ol> </li> <li>Python (Microsoft's bundle of Python linters and formatters)<ol> <li>Configure the VS Code Python extension to use your virtual environment by choosing <code>Python: Select Interpreter</code> from the command palette and selecting your current virtual environment from the options.</li> </ol> </li> <li>sqlfluff (SQL linter)</li> </ol>"},{"location":"code/local-repo-setup/#5-install-pre-commit-hooks","title":"5. Install <code>pre-commit</code> hooks","text":"<p>This project uses pre-commit to lint, format, and generally enforce code quality. These checks are run on every commit, as well as in CI.</p> <p>To set up your pre-commit environment locally run the following in the repository root folder:</p> <pre><code>pre-commit install\n</code></pre> <p>The next time you make a commit, the pre-commit hooks will run on the contents of your commit (the first time may be a bit slow as there is some additional setup).</p> <p>You can verify that the pre-commit hooks are working properly by running the following snippet to test every file in the repository against the checks.</p> <pre><code>pre-commit run --all-files\n</code></pre>"},{"location":"code/local-repo-setup/#updating-package-versions","title":"Updating package versions","text":"<p>It's good for project hygiene to periodically update package versions of the project dependencies. This brings in security patches, bugfixes, performance enhancements, and new features. If a project dependency has a CVE published, then we recommend updating it as soon as possible, otherwise a more relaxed update cadence is okay (e.g., once every six months to a year).</p> <p>When you do update, follow these steps:</p> <ol> <li>Check your Python version in <code>.python-version</code>. If that version has reached (or is near)     end-of-life, upgrade to a more recent version.</li> <li>Check the versions of the packages <code>dependencies</code> and the <code>dev</code> dependency group against     their most recent major version in PyPI, and update as appropriate.     (Note that the <code>~</code> operator     can be helpful in allowing some wiggle room in versions).</li> <li>Run <code>uv lock</code> to update the <code>uv.lock</code> file, and commit it.</li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/","title":"dbt Cloud practice","text":""},{"location":"data-transformation/dbt-cloud-practice/#day-1","title":"Day 1","text":""},{"location":"data-transformation/dbt-cloud-practice/#tour-of-dbt-cloud-user-interface","title":"Tour of dbt Cloud user interface","text":"<ol> <li>We\u2019ll give you a brief overview of the dbt Cloud user interface:<ol> <li>Studio \u2192 File explorer \u2192 File editor \u2192 Preview \u2192 Results \u2192 Lineage</li> </ol> </li> <li>Validate your development environment:<ol> <li>Open the Develop tab in your own environment and open <code>transform/models/staging/training/stg_water_quality__stations.sql</code></li> <li>Click on the Preview button. You should see data in the lower panel</li> </ol> </li> <li>Demonstrate the Lint/Fix functionality</li> <li>Demonstrate the Build/Test functionality</li> <li>Demonstrate <code>dbt run --select &lt;model name&gt;</code> on the command line</li> <li>Verify that the models you built/ran are visible in your personal schema within <code>TRANSFORM_DEV</code></li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/#practice-a-create-your-first-dbt-staging-model-for-the-stations-data","title":"Practice A: Create your first dbt staging model for the <code>STATIONS</code> data","text":"<p>Let\u2019s create two staging models! The data in <code>raw_dev.water_quality.stations</code> and <code>raw_dev.water_quality.lab_results</code> have been loaded from data.ca.gov/dataset/water-quality-data without modification except for the exclusion of the <code>_id</code> column in each table. There are a few simple transformations we can do to make working with these data more ergonomic. Models that require simple transformations involving things like data type conversion or column renaming are called staging models.</p> <ol> <li>Find and switch to your branch: <code>&lt;your-first-name&gt;-dbt-training</code></li> <li>Open <code>transform/models/staging/training/stg_water_quality__stations.sql</code> \u2013 you should see a SQL statement that selects all of the data from the raw table</li> <li>Update the select statement to do the following:<ol> <li>Explicitly select all columns by name rather than with <code>*</code></li> <li>Exclude the following column: <code>STATION_NAME</code></li> <li>Change the <code>STATION_ID</code> column type to varchar<ol> <li>Use Snowflake\u2019s TO_VARCHAR() function which needs one argument \u2013 the column to be converted</li> </ol> </li> <li>Change the <code>SAMPLE_DATE_MIN</code> and <code>SAMPLE_DATE_MAX</code> columns to timestamps and rename them to <code>SAMPLE_TIMESTAMP_MIN</code> and <code>SAMPLE_TIMESTAMP_MAX</code><ol> <li>Use Snowflake\u2019s TO_TIMESTAMP() function which needs two arguments \u2013 the column to be converted and the output format e.g. <code>YYYY-MM-DD HH24:MI:SS</code></li> </ol> </li> <li>Structure your query so that the main part of it is in a CTE, from which you <code>select *</code> at the end</li> </ol> </li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/#practice-b-create-your-second-staging-model-for-the-lab_results-data","title":"Practice B: Create your second staging model for the <code>LAB_RESULTS</code> data","text":"<ol> <li>Remain on your current branch: <code>&lt;your-first-name&gt;-dbt-training</code></li> <li>Open <code>transform/models/staging/training/stg_water_quality__lab_results.sql</code> \u2013 you should see a SQL statement that selects all of the data from the raw table</li> <li>Update the select statement to do the following:<ol> <li>Explicitly select the following columns by name rather than with <code>*</code>:<ul> <li><code>station_id</code>, <code>status</code>, <code>sample_code</code>, <code>sample_date</code>, <code>sample_depth</code>, <code>sample_depth_units</code>, <code>parameter</code>, <code>result</code>, <code>reporting_limit</code>, <code>units</code>, and <code>method_name</code></li> </ul> </li> <li>Change the <code>station_id</code> column type to varchar<ol> <li>Use Snowflake\u2019s TO_VARCHAR() function which needs one argument \u2013 the column to be converted</li> </ol> </li> <li>The <code>sample_date</code> column in the source data table is of data type <code>VARCHAR</code> and we want to change it to <code>DATE</code>. The values for this column are also formatted like timestamps. We want this column to both be of type <code>DATE</code> and contain values that look like dates.<ol> <li>Use Snowflake's DATE_FROM_PARTS() function to extract the parts of this column needed to turn it into a date. You'll need to use other string manipulation functions as well e.g. SUBSTR(), LEFT(), RIGHT(). And cast the values from those resulting parts as <code>INT</code> before feeding them into the date_from_parts function. This column should still be aliased as SAMPLE_DATE.</li> </ol> </li> <li>Change the <code>sample_date</code> column type again, to timestamp and rename it to SAMPLE_TIMESTAMP<ol> <li>Use Snowflake\u2019s TO_TIMESTAMP() function which needs two arguments \u2013 the column to be converted and the output format e.g. <code>YYYY-MM-DD HH24:MI:SS</code></li> </ol> </li> </ol> </li> <li>Structure your query so that the main part of it is in a CTE, from which you <code>select *</code> at the end</li> </ol> <p>Final instructions</p> <ol> <li>Lint and Fix your files, save any changes made</li> <li>Commit and sync your code and leave a concise, yet descriptive commit message</li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/#day-2","title":"Day 2","text":""},{"location":"data-transformation/dbt-cloud-practice/#practice-a-write-yaml-for-your-source-data-and-staging-models","title":"Practice A: Write YAML for your source data and staging models","text":"<p>Here you\u2019ll write YAML configuration for the Water Quality source tables, and for the two staging models you built. It will build on the branch you created in the previous exercise, so open dbt Cloud, navigate to the developer tab, and make sure that branch is checked out.</p> <ol> <li>Switch to your existing branch: <code>&lt;your-first-name&gt;-dbt-training</code></li> <li>Open <code>transform/models/_sources.yml</code>. You should see mostly empty stubs for models and sources.</li> <li>First, specify where the Water Quality data exists in the Snowflake database. We\u2019ll do that by adding some keys to the <code>Water Quality</code> source:<ol> <li>Add a key for the database: (<code>database: RAW_DEV</code>).</li> <li>Add a key for the schema: (<code>schema: WATER_QUALITY</code>).</li> </ol> </li> <li> <p>Describe the tables that exist in the WATER_QUALITY schema:</p> <pre><code>sources:\n  - name: WATER_QUALITY\n    database: &lt;database name here&gt;\n    schema: &lt;schema name here&gt;\n    description: &lt;data description here&gt;\n    tables:\n      - name: &lt;table name here&gt;\n        description: &lt;table description here&gt;\n        columns:\n          - name: &lt;column name here&gt;\n            description: &lt;column description here&gt;\n            name: &lt;column name here&gt;\n            description: &lt;column description here&gt;\n            \u2026\u00a0 # etc\n</code></pre> </li> <li> <p>Format your file, save any changes made</p> </li> <li>Open <code>transform/models/staging/training/stg_water_quality__stations.sql</code> and change the reference to our source data by using the <code>source()</code> macro we just learned about instead of directly referring to the table name</li> <li>Lint and Fix your file, save any changes made</li> <li>Commit and sync your code and leave a concise, yet descriptive commit message</li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/#practice-b-write-tests-for-the-stg_water_quality__stations-model","title":"Practice B: Write tests for the <code>stg_water_quality__stations</code> model","text":"<p>Open your <code>transform/models/staging/training/_water_quality.yml</code> and write some data integrity tests for your <code>stg_water_quality__stations</code> model.</p> <ol> <li>Add a not null test for STATION_ID</li> <li>Add a unique test for COUNTY_NAME. This one should fail!</li> <li>In your dbt Cloud command line, run <code>dbt test --select stg_water_quality__stations</code></li> </ol> <p>Note</p> <p>The grain at which the stations data is collected results in duplicate county names so this is not a good test for this column.</p>"},{"location":"data-transformation/dbt-cloud-practice/#day-3","title":"Day 3","text":""},{"location":"data-transformation/dbt-cloud-practice/#practice-create-and-document-an-intermediate-dbt-model","title":"Practice: Create and document an intermediate dbt model","text":"<p>Now that we\u2019ve gotten some practice creating two staging models and editing our YAML file to reference our source data and models, let's create an intermediate model and update the relevant YAML file.</p> <p>SQL:</p> <ol> <li>Switch to your existing branch: <code>&lt;your-first-name&gt;-dbt-training</code></li> <li>Open <code>transform/models/intermediate/training/int_water_quality__stations_per_county_with_parameter_2023_counted.sql</code></li> <li>Change the reference to the staging model by using the <code>ref()</code> macro we learned about</li> <li>Write a SQL query to return a count of the stations per county that reported a parameter of Dissolved Chloride for the year 2023 sorted from greatest to least.</li> <li>Hints<ol> <li>This will make use of a SQL group by, aggregation, and join</li> <li>Your output table should have two columns</li> <li>Use Snowflake\u2019s year() function</li> </ol> </li> <li>Structure your query so that the main part of it is in a CTE, from which you <code>select *</code> at the end</li> <li>Lint and Fix your file, save any changes made</li> </ol> <p>YAML:</p> <ol> <li>Document your new intermediate model in the <code>transform/models/intermediate/training/_water_quality.yml</code> file</li> <li>Materialize your model as a table</li> <li>Format your file, save any changes made</li> </ol> <p>Pull Request:</p> <ol> <li>Commit and sync your code and leave a concise, yet descriptive commit message</li> <li>In GitHub or Azure DevOps, create a PR</li> <li>Check that you\u2019ve added a teammate as a reviewer to your PR</li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/#day-4","title":"Day 4","text":""},{"location":"data-transformation/dbt-cloud-practice/#practice-a-custom-schemas","title":"Practice A: Custom schemas","text":"<ol> <li>Configure your intermediate model to build in a custom schema called <code>statistics</code>. You can do this by creating a new property in the model YAML config block: \u201c<code>schema: statistics</code>\u201d.</li> <li>Build your model and find it in Snowflake.</li> </ol>"},{"location":"data-transformation/dbt-cloud-practice/#practice-b-get-your-branch-to-pass-ci-checks","title":"Practice B: Get your branch to pass CI checks","text":"<p>You\u2019ve been working in your own branches to create dbt models and configuration files. Ultimately, our goal is to develop production-grade models, which are documented, configured, and passing CI.</p> <p>If using GitHub:</p> <ol> <li>Inspect the <code>pre-commit</code> results of your pull request in GitHub.</li> <li>Address any issues flagged by the results. Remember, the \u201cFormat\u201d, \u201cLint\u201d, and \u201cFix\u201d buttons in dbt Cloud can help with auto-resolving issues around formatting.</li> <li>Inspect the dbt Cloud test results in GitHub. Resolve any issues with your models not building or failing data integrity tests.</li> <li>Request a review of a teammate. Review another teammate\u2019s PR.</li> <li>Address any comments or suggestions from your code review.</li> <li>Repeat the above steps until there are no remaining comments, and you get a green checkmark on the CI checks!</li> </ol> <p>If using Azure DevOps:</p> <ol> <li>On the page for your pull request you should see the results of the CI checks. If it\u2019s green, great, it passed!</li> <li>If the CI check is red, click on it to see the logs, which will give more information about the failure. You might see failures in:<ol> <li>The linter checks (which looks for code style and common gotchas).</li> <li>Model builds, which indicate some logic issue in the code.</li> <li>Data tests, which ensure that the data has the shape you expect.</li> </ol> </li> <li>Address any issues flagged by the check. Remember, the \u201cFormat\u201d, \u201cLint\u201d, and \u201cFix\u201d buttons in dbt Cloud can help with auto-resolving issues around formatting.</li> <li>Request a review of a teammate. Review another teammate\u2019s PR.</li> <li>Address any comments or suggestions from your code review.</li> <li>Repeat the above steps until there are no remaining comments, and you get a green checkmark on the CI checks.</li> </ol>"},{"location":"data-transformation/dbt-core-practice/","title":"dbt Core exercises","text":""},{"location":"data-transformation/dbt-core-practice/#day-1","title":"Day 1","text":""},{"location":"data-transformation/dbt-core-practice/#dbt-core-command-line-reference","title":"dbt core command line reference","text":"<ol> <li>Build and test all models in the project: <code>dbt build</code></li> <li>Build and tests a specific model: <code>dbt build --select path/to/the/model.sql</code></li> <li>Build a specific model and its upstream dependencies: <code>dbt build --select +path/to/the/model.sql</code></li> <li>Build a specific model and its downstream dependencies: <code>dbt build --select path/to/the/model.sql+</code></li> <li>Build all the models in a specific directory: <code>dbt build --select /path/to/the/directory</code></li> <li>Test a model: <code>dbt test --select path/to/the/model.sql</code></li> <li>Test a model and its upstream and downstream dependencies: <code>dbt test --select +path/to/the/model.sql+</code></li> <li>Build, but do not test, all models: <code>dbt run</code></li> </ol>"},{"location":"data-transformation/dbt-core-practice/#practice-a-create-your-first-dbt-staging-model-for-the-stations-data","title":"Practice A: Create your first dbt staging model for the <code>STATIONS</code> data","text":"<p>Let\u2019s create two staging models! The data in <code>raw_dev.water_quality.stations</code> and <code>raw_dev.water_quality.lab_results</code> have been loaded from data.ca.gov/dataset/water-quality-data without modification except for the exclusion of the <code>_id</code> column in each table. There are a few simple transformations we can do to make working with these data more ergonomic. Models that require simple transformations involving things like data type conversion or column renaming are called staging models.</p> <ol> <li>Switch to your branch: <code>git switch &lt;your-first-name&gt;-dbt-training</code> (we created this in advance for you!)</li> <li>In your text editor, open <code>transform/models/staging/training/stg_water_quality__stations.sql</code> \u2013 you should see a SQL statement that selects all of the data from the raw table</li> <li>Update the select statement to do the following:<ol> <li>Explicitly select all columns by name rather than with <code>*</code></li> <li>Exclude the following column: <code>STATION_NAME</code></li> <li>Change the <code>STATION_ID</code> column type to varchar<ol> <li>Use Snowflake\u2019s TO_VARCHAR() function which needs one argument \u2013 the column to be converted</li> </ol> </li> <li>Change the <code>SAMPLE_DATE_MIN</code> and <code>SAMPLE_DATE_MAX</code> columns to timestamps and rename them to <code>SAMPLE_TIMESTAMP_MIN</code> and <code>SAMPLE_TIMESTAMP_MAX</code><ol> <li>Use Snowflake\u2019s TO_TIMESTAMP() function which needs two arguments \u2013 the column to be converted and the output format e.g. <code>YYYY-MM-DD HH24:MI:SS</code></li> </ol> </li> <li>Structure your query so that the main part of it is in a CTE, from which you <code>select *</code> at the end</li> </ol> </li> </ol>"},{"location":"data-transformation/dbt-core-practice/#practice-b-create-your-second-staging-model-for-the-lab_results-data","title":"Practice B: Create your second staging model for the <code>LAB_RESULTS</code> data","text":"<ol> <li>Remain on your current branch: <code>&lt;your-first-name&gt;-dbt-training</code></li> <li>Open <code>transform/models/staging/training/stg_water_quality__lab_results.sql</code> \u2013 you should see a SQL statement that selects all of the data from the raw table</li> <li>Update the select statement to do the following:<ol> <li>Explicitly select the following columns by name rather than with <code>*</code>:<ul> <li><code>station_id</code>, <code>status</code>, <code>sample_code</code>, <code>sample_date</code>, <code>sample_depth</code>, <code>sample_depth_units</code>, <code>parameter</code>, <code>result</code>, <code>reporting_limit</code>, <code>units</code>, and <code>method_name</code></li> </ul> </li> <li>Change the <code>station_id</code> column type to varchar<ol> <li>Use Snowflake\u2019s TO_VARCHAR() function which needs one argument \u2013 the column to be converted</li> </ol> </li> <li>The <code>sample_date</code> column in the source data table is of data type <code>VARCHAR</code> and we want to change it to <code>DATE</code>. The values for this column are also formatted like timestamps. We want this column to both be of type <code>DATE</code> and contain values that look like dates.<ol> <li>Use Snowflake's DATE_FROM_PARTS() function to extract the parts of this column needed to turn it into a date. You'll need to use other string manipulation functions as well e.g. SUBSTR(), LEFT(), RIGHT(). And cast the values from those resulting parts as <code>INT</code> before feeding them into the date_from_parts function. This column should still be aliased as SAMPLE_DATE.</li> </ol> </li> <li>Change the <code>sample_date</code> column type again, to timestamp and rename it to SAMPLE_TIMESTAMP<ol> <li>Use Snowflake\u2019s TO_TIMESTAMP() function which needs two arguments \u2013 the column to be converted and the output format e.g. <code>YYYY-MM-DD HH24:MI:SS</code></li> </ol> </li> </ol> </li> <li>Structure your query so that the main part of it is in a CTE, from which you <code>select *</code> at the end</li> </ol> <p>Final instructions</p> <ol> <li>Lint and Format your files<ol> <li>You can lint your SQL files by navigating to the transform directory and running: <code>sqlfluff lint models/staging</code></li> <li>You can fix your SQL files (at least the things that are easy to fix) by remaining in the transform directory and running <code>sqlfluff fix models/staging</code><ol> <li>For things that could not be auto-fixed you'll have to manually do it.</li> </ol> </li> <li>Or, to run all the checks, run<code>pre-commit run --all-files</code> Note: we don't recommend running this at this stage, since crucial project set up fixes will be addressed in further exercises.</li> </ol> </li> </ol> <p>Any of the above steps may modify your files requiring you to save (<code>git add</code>) them again.</p> <ol> <li>Check to see which files need to be added or removed: <code>git status</code></li> <li>Add or remove any relevant files: <code>git add filename.ext</code> or <code>git rm filename.ext</code></li> <li>Commit your code and leave a concise, yet descriptive commit message: <code>git commit -m \"example message\"</code><ol> <li>During this step pre-commit may catch an error you missed. It may auto-fix your file or you may have to do it yourself. Regardless you will have to repeat <code>git add...</code> (for each modified file) and <code>git commit...</code>.</li> </ol> </li> <li>Push your code: <code>git push origin &lt;your-first-name&gt;-dbt-training</code></li> </ol>"},{"location":"data-transformation/dbt-core-practice/#day-2","title":"Day 2","text":""},{"location":"data-transformation/dbt-core-practice/#practice-a-write-yaml-for-your-source-data-and-staging-models","title":"Practice A: Write YAML for your source data and staging models","text":"<p>Here you\u2019ll write YAML configuration for the Water Quality source tables, and for the two staging models you built. It will build on the branch you created in the previous exercise, so open dbt Cloud, navigate to the developer tab, and make sure that branch is checked out.</p> <ol> <li>Switch to your working branch: <code>git switch &lt;your-first-name&gt;-dbt-training</code></li> <li>In your text editor, open <code>transform/models/_sources.yml</code>. You should see mostly empty stubs for models and sources.</li> <li>First, specify where the Water Quality data exists in the Snowflake database. We\u2019ll do that by adding some keys to the <code>Water Quality</code> source:<ol> <li>Add a key for the database: (<code>database: RAW_DEV</code>).</li> <li>Add a key for the schema: (<code>schema: WATER_QUALITY</code>).</li> </ol> </li> <li> <p>Describe the tables that exist in the WATER_QUALITY schema:</p> <pre><code>sources:\n  - name: WATER_QUALITY\n    database: &lt;database name here&gt;\n    schema: &lt;schema name here&gt;\n    description: &lt;data description here&gt;\n    tables:\n      - name: &lt;table name here&gt;\n        description: &lt;table description here&gt;\n        columns:\n          - name: &lt;column name here&gt;\n            description: &lt;column description here&gt;\n            name: &lt;column name here&gt;\n            description: &lt;column description here&gt;\n            \u2026\u00a0 # etc\n</code></pre> </li> <li> <p>Open <code>transform/models/staging/training/stg_water_quality__stations.sql</code> and change the reference to our source data by using the <code>source()</code> macro we just learned about instead of directly referring to the table name</p> </li> </ol>"},{"location":"data-transformation/dbt-core-practice/#practice-b-write-tests-for-the-stg_water_quality__stations-model","title":"Practice B: Write tests for the <code>stg_water_quality__stations</code> model","text":"<p>Open your <code>transform/models/staging/training/_stg_water_quality.yml</code> and write some data integrity tests for your <code>stg_water_quality__stations</code> model.</p> <ol> <li>Add a not null test for STATION_ID</li> <li>Add a unique test for COUNTY_NAME. This one should fail!</li> <li>In your dbt Cloud command line, run <code>dbt test --select stg_water_quality__stations</code></li> </ol> <p>Note</p> <p>The grain at which the stations data is collected results in duplicate county names so this is not a good test for this column.</p> <p>Final instructions</p> <ol> <li>Lint and Format your files<ol> <li>You can lint your SQL files by navigating to the transform directory and running: <code>sqlfluff lint</code></li> <li>You can fix your SQL files (at least the things that are easy to fix) by remaining in the transform directory and running <code>sqlfluff fix</code><ol> <li>For things that could not be auto-fixed you'll have to manually do it.</li> </ol> </li> <li>Or, to run all the checks, run<code>pre-commit run --all-files</code></li> </ol> </li> </ol> <p>Any of the above steps may modify your files requiring you to save (<code>git add</code>) them again.</p> <ol> <li>Check to see which files need to be added or removed: <code>git status</code></li> <li>Add or remove any relevant files: <code>git add filename.ext</code> or <code>git rm filename.ext</code></li> <li>Commit your code and leave a concise, yet descriptive commit message: <code>git commit -m \"example message\"</code><ol> <li>During this step pre-commit may catch an error you missed. It may auto-fix your file or you may have to do it yourself. Regardless you will have to repeat <code>git add...</code> (for each modified file) and <code>git commit...</code>.</li> </ol> </li> <li>Push your code to your working branch: <code>git push origin &lt;your-first-name&gt;-dbt-training</code></li> </ol>"},{"location":"data-transformation/dbt-core-practice/#day-3","title":"Day 3","text":""},{"location":"data-transformation/dbt-core-practice/#practice-create-and-document-an-intermediate-dbt-model","title":"Practice: Create and document an intermediate dbt model","text":"<p>Now that we\u2019ve gotten some practice creating two staging models and editing our YAML file to reference our source data and models, let's create an intermediate model and update the relevant YAML file.</p> <p>SQL:</p> <ol> <li>Switch to your working branch: <code>&lt;your-first-name&gt;-dbt-training</code></li> <li>Open <code>transform/models/intermediate/training/int_water_quality__stations_per_county_with_parameter_2023_counted.sql</code></li> <li>Change the reference to the staging model by using the <code>ref()</code> macro we learned about</li> <li>Write a SQL query to return a count of the stations per county that reported a parameter of Dissolved Chloride for the year 2023 sorted from greatest to least.</li> <li>Hints<ol> <li>This will make use of a SQL group by, aggregation, and join</li> <li>Your output table should have two columns</li> <li>Use Snowflake\u2019s year() function</li> </ol> </li> <li>Structure your query so that the main part of it is in a CTE, from which you <code>select *</code> at the end</li> </ol> <p>YAML:</p> <ol> <li>Document your new intermediate model in the <code>transform/models/intermediate/training/_int_water_quality.yml</code> file</li> <li>Materialize your model as a table</li> </ol> <p>Final instructions</p> <ol> <li>Lint and Format your files<ol> <li>You can lint your SQL files by navigating to the transform directory and running: <code>sqlfluff lint</code></li> <li>You can fix your SQL files (at least the things that are easy to fix) by remaining in the transform directory and running <code>sqlfluff fix</code><ol> <li>For things that could not be auto-fixed you'll have to manually do it.</li> </ol> </li> <li>Or, to run all the checks, run<code>pre-commit run --all-files</code></li> </ol> </li> </ol> <p>Any of the above steps may modify your files requiring you to save (<code>git add</code>) them again.</p> <ol> <li>Check to see which files need to be added or removed: <code>git status</code></li> <li>Add or remove any relevant files: <code>git add filename.ext</code> or <code>git rm filename.ext</code></li> <li>Commit your code and leave a concise, yet descriptive commit message: <code>git commit -m \"example message\"</code><ol> <li>During this step pre-commit may catch an error you missed. It may auto-fix your file or you may have to do it yourself. Regardless you will have to repeat <code>git add...</code> (for each modified file) and <code>git commit...</code>.</li> </ol> </li> <li>Push your code to your working branch: <code>git push origin &lt;your-first-name&gt;-dbt-training</code></li> </ol>"},{"location":"data-transformation/dbt-core-practice/#day-4","title":"Day 4","text":""},{"location":"data-transformation/dbt-core-practice/#practice-a-custom-schemas","title":"Practice A: Custom schemas","text":"<ol> <li>Configure your intermediate model to build in a custom schema called <code>statistics</code>. You can do this by creating a new property in the model YAML config block: \u201c<code>schema: statistics</code>\u201d.</li> <li>Build your model and find it in Snowflake.</li> </ol> <p>Final instructions</p> <ol> <li>Lint and Format your files<ol> <li>You can lint your SQL files by navigating to the transform directory and running: <code>sqlfluff lint</code></li> <li>You can fix your SQL files (at least the things that are easy to fix) by remaining in the transform directory and running <code>sqlfluff fix</code><ol> <li>For things that could not be auto-fixed you'll have to manually do it.</li> </ol> </li> <li>Or, to run all the checks, run<code>pre-commit run --all-files</code></li> </ol> </li> </ol> <p>Any of the above steps may modify your files requiring you to save (<code>git add</code>) them again.</p> <ol> <li>Check to see which files need to be added or removed: <code>git status</code></li> <li>Add or remove any relevant files: <code>git add filename.ext</code> or <code>git rm filename.ext</code></li> <li>Commit your code and leave a concise, yet descriptive commit message: <code>git commit -m \"example message\"</code><ol> <li>During this step pre-commit may catch an error you missed. It may auto-fix your file or you may have to do it yourself. Regardless you will have to repeat <code>git add...</code> (for each modified file) and <code>git commit...</code>.</li> </ol> </li> <li>Push your code to your working branch: <code>git push origin &lt;your-first-name&gt;-dbt-training</code></li> <li>Create a new pull request with your working branch</li> <li>Add a teammate as a reviewer</li> </ol>"},{"location":"data-transformation/dbt-core-practice/#practice-b-get-your-branch-to-pass-ci-checks","title":"Practice B: Get your branch to pass CI checks","text":"<p>You\u2019ve been working in your own branches to create dbt models and configuration files. Ultimately, our goal is to develop production-grade models, which are documented, configured, and passing CI. We\u2019ll end the day by live reviewing a PR.</p>"},{"location":"data-transformation/dbt-practice-answers/","title":"Training answer key","text":""},{"location":"data-transformation/dbt-practice-answers/#day-1","title":"Day 1","text":""},{"location":"data-transformation/dbt-practice-answers/#answer-for-practice-a-create-your-first-dbt-staging-model-for-the-stations-data","title":"Answer for Practice A: Create your first dbt staging model for the <code>STATIONS</code> data","text":"<pre><code>with stations as (\n\n    select\n        to_varchar(station_id) as station_id,\n\n        full_station_name,\n        station_number,\n        station_type,\n        latitude,\n        longitude,\n        county_name,\n        sample_count,\n\n        to_timestamp(sample_date_min, 'MM/DD/YYYY HH24:MI') as sample_timestamp_min,\n        to_timestamp(sample_date_max, 'MM/DD/YYYY HH24:MI') as sample_timestamp_max\n\n    from {{ source('WATER_QUALITY', 'STATIONS') }}\n)\n\nselect * from stations\n</code></pre>"},{"location":"data-transformation/dbt-practice-answers/#answer-for-practice-b-create-your-second-staging-model-for-the-lab_results-data","title":"Answer for Practice B: Create your second staging model for the <code>LAB_RESULTS</code> data","text":"<pre><code>with lab_results as (\n\n    select\n        to_varchar(\"station_id\") as station_id,\n        \"status\" as status,\n        \"sample_code\" as sample_code,\n\n        to_timestamp(\"sample_date\", 'MM/DD/YYYY HH24:MI') as sample_timestamp,\n\n        date_from_parts(\n            substr(\"sample_date\", 7, 4)::INT,\n            left(\"sample_date\", 2)::INT,\n            substr(\"sample_date\", 4, 2)::INT\n        ) as sample_date,\n\n        sample_depth,\n        sample_depth_units,\n        parameter,\n        result,\n        reporting_limit,\n        units,\n        method_name\n\n    from {{ source('WATER_QUALITY', 'LAB_RESULTS') }}\n)\n\nselect * from lab_results\n</code></pre>"},{"location":"data-transformation/dbt-practice-answers/#day-2","title":"Day 2","text":""},{"location":"data-transformation/dbt-practice-answers/#answers-for-practice-a-and-b","title":"Answers for Practice A and B","text":"<p>Reminder:</p> <ul> <li>Practice A: Write YAML for your source data and staging models</li> <li>Practice B: Write tests for the <code>stg_water_quality__stations</code> model</li> </ul> <p>YAML for your source data</p> <pre><code>version: 2\n\nsources:\n  - name: WATER_QUALITY\n    database: RAW_DEV\n    schema: WATER_QUALITY\n    description: |\n      The California Department of Water Resources (DWR) discrete (vs. continuous)\n      water quality datasets contains DWR-collected, current and historical, chemical\n      and physical parameters found in routine environmental, regulatory compliance\n      monitoring, and special studies throughout the state.\n    tables:\n      - name: LAB_RESULTS\n        description: The source data for lab results.\n        columns:\n          - name: station_id\n            description: A unique identifier for stations.\n          - name: station_name\n            description: |\n              Abbreviated Long Station Name or a unique code\n              assigned to the sampling location. Limit 20 characters.\n          - name: full_station_name\n            description: |\n              The (full) station name describing the sampling location\n              based on DWR station naming conventions.\n          - name: station_number\n            description: |\n              Unique DWR station code based on DWR station numbering\n              conventions.\n          - name: station_type\n            description: |\n              General description of sampling site location,\n              i.e., surface water, groundwater, or other.\n          - name: latitude\n            description: Latitude (NAD83).\n          - name: longitude\n            description: Longitude (NAD83).\n          - name: status\n            description: Data review status.\n          - name: county_name\n            description: County where sample collected.\n          - name: sample_code\n            description: Unique DWR lab and field data sample code.\n          - name: sample_date\n            description: The date and time a sample was collected.\n          - name: sample_depth\n            description: The depth below the water surface at which the sample was collected.\n          - name: sample_depth_units\n            description: The unit of measurement of sample_depth, e.g. feet\n          - name: parameter\n            description: The chemical analyte or physical parameter that was measured.\n          - name: result\n            description: The measured result of the constituent.\n          - name: reporting_limit\n            description: The lowest quantifiable detection limit of measure.\n          - name: units\n            description: Units of measure for the result.\n          - name: method_name\n            description: The analytical method by which the constituent was measured.\n      - name: STATIONS\n        description: The source data for stations.\n        columns:\n          - name: station_id\n            description: A unique identifier for stations.\n          - name: station_name\n            description: |\n              Abbreviated Long Station Name or a unique code\n              assigned to the sampling location. Limit 20 characters.\n          - name: full_station_name\n            description: |\n              The (full) station name describing the sampling location\n              based on DWR station naming conventions.\n          - name: station_number\n            description: |\n              Unique DWR station code based on DWR station numbering\n              conventions.\n          - name: station_type\n            description: |\n              General description of sampling site location,\n              i.e., surface water, groundwater, or other.\n          - name: latitude\n            description: Latitude (NAD83).\n          - name: longitude\n            description: Longitude (NAD83).\n          - name: county_name\n            description: County where sample collected.\n          - name: sample_code\n            description: Unique DWR lab and field data sample code.\n          - name: sample_date_min\n            description: |\n              Date of the first sample collection event on record\n              for a given DWR sampling location.\n          - name: sample_date_max\n            description: |\n              Date of the last sample collection event on record\n              for a given DWR sampling location.\n</code></pre> <p>YAML for your staging models</p> <pre><code>version: 2\n\nmodels:\n  - name: stg_water_quality__stations\n    description: Staging model for stations.\n    config:\n      materialized: table\n    data_tests:\n      - dbt_utils.equal_rowcount:\n          compare_model: source('WATER_QUALITY', 'STATIONS')\n    columns:\n      - name: station_id\n        description: A unique identifier for stations.\n        data_tests:\n          - not_null\n      - name: full_station_name\n        description: |\n          The (full) station name describing the sampling location\n          based on DWR station naming conventions.\n      - name: station_number\n        description: |\n          Unique DWR station code based on DWR station numbering\n          conventions.\n      - name: station_type\n        description: |\n          General description of sampling site location,\n          i.e., surface water, groundwater, or other.\n      - name: latitude\n        description: Latitude (NAD83).\n      - name: longitude\n        description: Longitude (NAD83).\n      - name: county_name\n        description: County where sample collected.\n        data_tests:\n          - unique\n      - name: sample_code\n        description: Unique DWR lab and field data sample code.\n      - name: sample_date_min\n        description: |\n          Date of the first sample collection event on record\n          for a given DWR sampling location.\n      - name: sample_date_max\n        description: |\n          Date of the last sample collection event on record\n          for a given DWR sampling location.\n  - name: stg_water_quality__lab_results\n    description: Statging model for lab results.\n    columns:\n      - name: station_id\n        description: A unique identifier for stations.\n      - name: status\n        description: Data review status.\n      - name: sample_code\n        description: Unique DWR lab and field data sample code.\n      - name: sample_timestamp\n        description: The date and time a sample was collected.\n      - name: sample_date\n        description: The date a sample was collected.\n      - name: sample_depth\n        description: The depth below the water surface at which the sample was collected.\n      - name: sample_depth_units\n        description: The unit of measurement of sample_depth, e.g. feet\n      - name: parameter\n        description: The chemical analyte or physical parameter that was measured.\n      - name: result\n        description: The measured result of the constituent.\n      - name: reporting_limit\n        description: The lowest quantifiable detection limit of measure.\n      - name: units\n        description: Units of measure for the result.\n      - name: method_name\n        description: The analytical method by which the constituent was measured.\n</code></pre>"},{"location":"data-transformation/dbt-practice-answers/#day-3","title":"Day 3","text":""},{"location":"data-transformation/dbt-practice-answers/#answer-for-practice-create-and-document-an-intermediate-dbt-model","title":"Answer for Practice: Create and document an intermediate dbt model","text":"<p>SQL</p> <pre><code>with stations as (\n    select * from {{ ref('stg_water_quality__stations') }}\n),\n\nlab_results as (\n    select * from {{ ref('stg_water_quality__lab_results') }}\n),\n\nstations_per_county as (\n    select\n        station_id,\n        county_name\n\n    from stations\n    group by station_id, county_name\n),\n\nstations_per_county_with_parameter_2023_counted as (\n    select\n        s.county_name,\n        count(distinct l.station_id) as station_count\n\n    from stations_per_county as s\n    inner join lab_results as l\n        on s.station_id = l.station_id\n\n    where\n        year(l.sample_date) = 2023\n        and l.parameter = 'Dissolved Chloride'\n    group by s.county_name\n)\n\nselect * from stations_per_county_with_parameter_2023_counted\norder by station_count desc\n</code></pre> <p>YAML</p> <pre><code>version: 2\n\nmodels:\n  - name: int_water_quality__stations_per_county_with_parameter_2023_counted\n    description: |\n      This model returns a count of the stations per county that\n      reported a parameter of Dissolved Chloride for the year\n      2023 sorted from greatest to least.\n    columns:\n      - name: county_name\n        description: County where sample collected.\n      - name: station_count\n        description: Count of stations that reported a parameter of Dissolved Chloride.\n</code></pre>"},{"location":"data-transformation/dbt-practice-answers/#day-4","title":"Day 4","text":"<pre><code>version: 2\n\nmodels:\n  - name: int_water_quality__stations_per_county_with_parameter_2023_counted\n    config:\n      schema: statistics\n    ...\n</code></pre>"},{"location":"data-transformation/dbt/","title":"dbt (data build tool)","text":""},{"location":"data-transformation/dbt/#day-1","title":"Day 1","text":""},{"location":"data-transformation/dbt/#what-is-dbt","title":"What is dbt?","text":"<p>dbt is a SQL-first transformation tool that lets teams quickly and collaboratively deploy analytics code while following software engineering best practices like modularity, version control, CI/CD, and documentation. This allows your data team to safely develop and contribute to production-grade data pipelines.</p>"},{"location":"data-transformation/dbt/#data-modeling-in-the-context-of-dbt","title":"Data modeling in the context of dbt","text":"<p>In the context of dbt, data modeling refers to the process of organizing data in a structured and efficient manner to facilitate data analysis and decision-making. Data models in dbt serve as blueprints for transforming and organizing your raw data into valuable insights. Data models in their final form are usually a representation of a business or program area and live as tables or views in your data warehouse.</p>"},{"location":"data-transformation/dbt/#data-layers-staging-intermediate-marts","title":"Data layers (staging, intermediate, marts)","text":"<p>Data layers represent a systematic approach to data modeling by organizing data into distinct phases. dbt does a particularly great job of explaining best practices to structuring your project and data with naming conventions, example code, and reasoning on such practices in this guide. We\u2019ve summarized it below, but still recommend a thorough read of dbt\u2019s guide from which the content below stems.</p> <ol> <li>Staging<ol> <li>The staging layer is the initial point of contact for your raw data</li> <li>Models in the staging layer have a one-to-one relationship with source data ensuring data integrity and providing a reliable foundation for downstream models</li> <li>There are very few transformations that happen in this layer. Appropriate ones are:<ol> <li>Column renaming (e.g. PLACEFP to place_fips)</li> <li>Data type casting (e.g. string type to numeric)</li> <li>Basic computations (e.g. cents to dollars)</li> </ol> </li> <li>Staging models are often materialized ephemerally or as views (more about materializations later!)</li> <li>Files are prefixed with <code>stg_</code> and saved in a subdirectory usually named \u201cstaging\u201d of the models folder</li> </ol> </li> <li>Intermediate<ol> <li>Intermediate models are where you start applying more complex transformations to your data</li> <li>This layer is used for data cleansing, feature engineering, and combining data from different sources</li> <li>Intermediate models allow you to build modular and reusable transformations following the principles of DRY (Don\u2019t Repeat Yourself)</li> <li>Common transformations that happen in this layer are:<ol> <li>Table joins or unions</li> <li>Data aggregations (e.g. using a function like <code>SUM()</code>)</li> <li>Data pivots</li> </ol> </li> <li>Files are prefixed with <code>int_</code> and saved in a subdirectory usually named \u201cintermediate\u201d of the models folder</li> </ol> </li> <li>Marts (or Data Marts)<ol> <li>Marts are the final layer in the data modeling process, representing consumable datasets tailored for specific business or programs needs</li> <li>Sometimes called the entity layer or concept layer, to emphasize that our marts are meant to represent a specific entity or concept at its unique grain</li> <li>Use plain English to name the file based on the concept that forms the grain of the mart e.g. <code>incidents.sql</code>, <code>claimants.sql</code>, <code>orders.sql</code></li> <li>Wide and denormalized</li> <li>Files are saved in a subdirectory usually named \"marts\" of the models folder</li> </ol> </li> </ol>"},{"location":"data-transformation/dbt/#common-table-expressions-ctes","title":"Common table expressions (CTEs)","text":"<p>CTEs are widely used as a way to create modular and readable SQL queries. You can think of CTEs as temporary, named data tables within your SQL queries. CTEs facilitate modularity and readability by encapsulating complex subqueries and making them reusable throughout your data models.</p> <p>Often CTEs are framed as an alternative to SQL subqueries. In dbt-style SQL, CTEs are usually preferable to subqueries for a few reasons:</p> <ol> <li>They allow you to read code from top to bottom rather than inside out</li> <li>They allow for better reuse of intermediate results</li> <li>They allow you to give descriptive names to intermediate results</li> </ol> Subquery CTE <pre>\n          <code>\nselect *\nfrom (\n    select\n        COUNTY_NAME,\n        SUM(SAMPLE_COUNT) as total_samples\n    from raw_dev.water_quality.stations\n    group by \"COUNTY_NAME\"\n) as subquery\nwhere total_samples &gt; 10\n          </code>\n        </pre> <pre>\n          <code>\nwith total_samples_by_county as (\n    select\n        \"COUNTY_NAME\",\n        SUM(SAMPLE_COUNT) as total_samples\n    from raw_dev.water_quality.stations\n    group by \"COUNTY_NAME\"\n)\nselect * from total_samples_by_county\nwhere total_samples &gt; 10\n          </code>\n        </pre>"},{"location":"data-transformation/dbt/#day-1-practice","title":"Day 1: Practice","text":"<p>Click either link for dbt Cloud or dbt Core practice.</p>"},{"location":"data-transformation/dbt/#day-1-references","title":"Day 1: References","text":""},{"location":"data-transformation/dbt/#dbt-fundamentals","title":"dbt Fundamentals","text":"<p>dbt Fundamentals is an online self-paced course on how to use dbt and dbt Cloud. It is broadly similar to the content in this training, and you may find some of the videos from the course helpful to review. We\u2019ve linked to some of the videos below.</p>"},{"location":"data-transformation/dbt/#models-in-dbt","title":"Models in dbt","text":"<ul> <li>What are models?</li> <li>Building your first model</li> <li>What is modularity?</li> </ul>"},{"location":"data-transformation/dbt/#day-2","title":"Day 2","text":""},{"location":"data-transformation/dbt/#what-are-all-these-yaml-files","title":"What are all these YAML files?!","text":"<p>Broadly speaking, there are two kinds of relations (a relation is a table or view) in a dbt project: \u201cmodels\u201d and \u201csources\u201d. \u201cSources\u201d are your raw, untransformed data sets. \u201cModels\u201d are the tables and views that you create when transforming your data into something. Both of these are described using YAML files.</p> <p>The YAML files in a dbt project contain the metadata for your relations, both sources and models. They include:</p> <ul> <li>Documentation</li> <li>Configuration</li> <li>Data tests</li> </ul>"},{"location":"data-transformation/dbt/#wait-but-what-is-yaml","title":"Wait, but what is YAML?","text":"<p>YAML stands for \u201cYet Another Markup Language\u201d. It is a superset of JSON (JavaScript Object Notation) and intended to be a more human readable version, but JSON is still perfectly valid! For example, <code>{\u201cmy-key\u201d: 4}</code> is YAML. YAML has support for:</p> <ul> <li>Key-value pairs (i.e., dictionaries/maps)</li> <li>Lists</li> <li>Strings, numbers, booleans</li> </ul> <p>It is also absolutely ubiquitous for tool configuration. Tools that are configured using YAML include:</p> <ul> <li>dbt</li> <li>GitHub Actions</li> <li>Azure Pipelines</li> <li>BitBucket Pipelines</li> <li>Kubernetes</li> <li>AWS CloudFormation</li> <li>Many more!</li> </ul> <p>Indentation is meaningful in YAML. Make sure that you use 2 spaces, rather than tab characters, to indent sections per level of nesting.</p>"},{"location":"data-transformation/dbt/#yaml-dictionariesmaps","title":"YAML dictionaries/maps","text":"<pre><code># In YAML, comments are started with the hashtag # symbol\n\n# Dictionaries/maps are constructed using indentation and the colon \":\" symbol\nmy_dictionary:\n  a_number: 12\n  a_string: \"hello, world!\"\n  a_boolean: true\n  a_nested_dictionary:\n    key: \"value\"\n    another_key: \"another value\"\n\n# Because YAML is a superset of JSON, we can equivalently write:\nmy_dictionary: {\"a_number\":12, \"a_string\": \"hello, world!\", \"a_boolean\": true}\n</code></pre>"},{"location":"data-transformation/dbt/#yaml-lists","title":"YAML lists","text":"<pre><code># Lists are constructed using the dash - symbol\nmy_list:\n  - 1\n  - 2\n  - 3\n  - 4\n\n# Because YAML is a superset of JSON, we can equivalently write:\nmy_list: [1, 2, 3, 4]\n</code></pre>"},{"location":"data-transformation/dbt/#yaml-strings","title":"YAML strings","text":"<pre><code># YAML strings may be written without quotes \"\" as long as there is no ambiguity\nmy_ionary:\n  a_string: \"britt is cool!\"\n  also_a_string: britt is cool\n\n# But omitting the quotes for a string can get you in trouble when the string is ambiguous!\nmy_dictionary:\n  # This is interpreted as a number (which is not what we want)\n  python_version: 3.9\n\n  # This would pull in python 3.1 instead of 3.10!\n  another_python_version: 3.10\n\n  # This gets interpreted as a boolean instead of a string!\n  should_i_eat_lunch: yes\n</code></pre>"},{"location":"data-transformation/dbt/#yaml-multiline-strings","title":"YAML multiline strings","text":"<pre><code># Use the pipe | or right angle bracket &gt; symbol to break up long strings for legibility\nlong_snippet: |\n  cotton candy blues\n  juxtaposed with blushing peaks\n  postcard-worthy views\n\nanother_long_snippet: &gt;\n  Call me Ishmael. Some years ago - never mind how long precisely -\n  having little or no money in my purse, and nothing particular to\n  interest me on shore, I thought I would sail about a little and see\n  the watery part of the world.\n</code></pre>"},{"location":"data-transformation/dbt/#markdown-in-yaml","title":"Markdown in YAML","text":"<pre><code># dbt renders description strings as Markdown\na_markdown_string: |\n  This is rendered as Markdown! So I can use _emphasis_\n  or **bold text**. I can also include:\n\n  - Lists\n  - of\n  - items\n\n  as well as [URLs](https://github.github.com/gfm/)\n</code></pre> <p>Note</p> <p>Code blocks are designed to display text literally, so Markdown formatting like bolding, italicizing, or headings won't be applied. Hence why you do not see the affect above. However, in dbt which does allow for richer text formatting, applying markdown to your YAML, like we've done above, will work.</p>"},{"location":"data-transformation/dbt/#sources-and-refs","title":"Sources and refs","text":"<p>Let\u2019s take a look at the <code>source</code> and <code>ref</code> dbt macros. Instead of directly referring to the database, schema, table, and view names, we use the <code>source</code> and <code>ref</code> dbt macros. The syntax for this is to replace the raw names with a template directive like this: <code>{{ source('WATER_QUALITY', 'STATIONS') }}</code>.</p> <p>The curly braces are a syntax for Jinja templating. The expression within the curly braces is a Python (ish) function which gets evaluated and inserted into the SQL file. There are lots of things we can do with Jinja to help generate our SQL queries, including basic math, custom Python functions, loops, and if-else statements. Most of the time, you will just need to be able to use the <code>source</code> and <code>ref</code> macros.</p>"},{"location":"data-transformation/dbt/#source","title":"source()","text":"<p>This function creates dependencies between source data and the current model (usually staging) referencing it. Your dbt project will depend on raw data stored in your database. Since this data is normally loaded by other tools than dbt, the structure of it can change over time \u2013 tables and columns may be added, removed, or renamed. When this happens, it is easier to update models if raw data is only referenced in one place.</p> <p>Example: replace <code>RAW_DEV.WATER_QUALITY.LAB_RESULTS</code> with <code>{{ source('WATER_QUALITY', 'LAB_RESULTS') }}</code>.</p>"},{"location":"data-transformation/dbt/#ref","title":"ref()","text":"<p>This function is how you reference a model from another: it allows you to build more complex models by referring to other ones and constructing a data lineage graph. Under the hood this function is actually doing two important things. First, it is interpolating the schema into your model file to allow you to change your deployment schema via configuration. Second, it is using these references between models to automatically build the dependency graph. This will enable dbt to deploy models in the correct order when using dbt run.</p> <p>Example: Replace <code>stg_water_quality__stations</code> with <code>{{ ref(\u2018stg_water_quality__stations\u2019) }}</code>.</p>"},{"location":"data-transformation/dbt/#why-shouldnt-we-directly-refer-to-table-names","title":"Why shouldn\u2019t we directly refer to table names?","text":"<p>It can be initially confusing to people that we don\u2019t directly refer to the names of the other data models, and instead do it indirectly via the <code>source</code> and <code>ref</code> macros. There are a few reasons for this:</p> <ol> <li>By explicitly linking your sources and models using the <code>source</code> and <code>ref</code> macros, you help dbt in constructing a data lineage graph (i.e., which tables depend on which others). This allows you to do things like \u201crebuild a model and all of its upstream dependencies\u201d, or \u201ctest a model and all of its downstream dependents\u201d.</li> <li>It becomes easier to rename a data source. This can be especially useful if the data source comes to you with unhelpful names.</li> <li>Source and refs become context aware. For example, in a development context, your personal development schema is templated into the SQL queries, but in a production context the final production schema is templated in. This allows for safer development of new models.</li> </ol>"},{"location":"data-transformation/dbt/#data-tests","title":"Data tests","text":"<p>dbt ships with 4 \u201cgeneric\u201d data tests. We recommend giving their data test documentation a thorough read as we only pulled high-level content from it.:</p> <ol> <li>Not null</li> <li>Unique</li> <li>Relationships</li> <li>Accepted Values</li> </ol> <p>Default test outcomes are:</p> <ul> <li>Pass: 0 rows found</li> <li>Fail: 1+ rows found</li> </ul> <p>dbt example:</p> <pre><code>models:\n  - name: orders\n    columns:\n      - name: order_id\n        data_tests:\n          - unique\n          - not_null\n      - name: status\n        data_tests:\n          - accepted_values:\n              values: ['placed', 'shipped', 'completed']\n      - name: customer_id\n        data_tests:\n          - relationships:\n              to: ref('customers')\n    data_tests:\n      - unique:\n          column_name: \"(customer_id || '-' || order_date)\"\n</code></pre> <p>In dbt you execute tests with either of the following:</p> <ul> <li>dbt test (tests models only)</li> <li>dbt build (builds and tests models)</li> </ul> <p>Sample results from the YAML config above</p> <pre><code>23:52:21  1 of 4 START test unique_orders__order_id_ ........... [RUN]\n23:52:21  2 of 4 START test not_null_orders__order_id_ ......... [RUN]\n23:52:21  3 of 4 START test accepted_values_orders__status_ .... [RUN]\n23:52:21  1 of 4 PASS unique_orders__order_id_ ................. [PASS in 0.69s]\n23:52:21  4 of 4 START test relationships_orders__customer_id_ . [RUN]\n23:52:22  2 of 4 PASS not_null_orders__order_id_ ............... [PASS in 0.78s]\n23:52:22  3 of 4 PASS accepted_values_orders__status_ .......... [PASS in 0.74s]\n23:52:22  4 of 4 PASS relationships_orders__customer_id_ ....... [PASS in 0.57s]\n23:52:22\n23:52:22  Finished running 4 tests in 0 hours 0 minutes and 2.81 seconds (2.81s).\n23:52:22\n23:52:22  Completed successfully\n23:52:22\n23:52:22  Done. PASS=4 WARN=0 ERROR=0 SKIP=0 TOTAL=4\n</code></pre> <p>Test severity can be configured like so:</p> <ul> <li>error: test will pass or fail (default)</li> <li>warn: test will pass or warn</li> </ul> <p>Tests with a severity of \u201cerror\u201d can also have warnings configured.</p> <p>dbt example:</p> <pre><code>columns:\n  - name: controller_id\n    data_tests:\n      - not_null\n\n  - name: controller_type\n    data_tests:\n      - not_null:\n          config:\n            severity: warn\n</code></pre> <p>Thresholds can be configured with error_if and warn_if options. The default is error_if:  <code>&gt; 0</code> When working with Snowflake you can use any supported comparison syntax like so: <code>= 0</code>, <code>&lt;&gt; 15</code>, or <code>between 10 and 20</code>. Also, note that error_if will be ignored if severity is warn!</p> <p>dbt example:</p> <pre><code>columns:\n  - name: controller_id\n    data_tests:\n      - not_null\n\n  - name: controller_type\n    data_tests:\n      - not_null:\n          config:\n            severity: warn\n\n  - name: district\n    data_tests:\n      - not_null:\n          config:\n            error_if: \"&gt;10\"\n            warn_if: \"&gt;5\"\n</code></pre> <p>For people who are familiar with transactional databases, you might be curious why tests like this are ever needed (i.e., why don\u2019t we handle it using constraints?). In a traditional transactional database like postgres or SQL Server, you can have a uniqueness constraint on a column. Snowflake does not respect uniqueness constraints and most OLAP databases do not either. Primary keys and foreign keys are examples of unique columns that are respected in OLTP databases that are not in OLAP databases. If you're interested the is more reading available on this topic.</p>"},{"location":"data-transformation/dbt/#day-2-practice","title":"Day 2: Practice","text":"<p>Click either link for dbt Cloud or dbt Core practice.</p>"},{"location":"data-transformation/dbt/#day-2-references","title":"Day 2: References","text":""},{"location":"data-transformation/dbt/#sources","title":"Sources","text":"<ul> <li>Modularity and ref functions</li> <li>What are sources?</li> <li>Configure and select from sources</li> <li>Documenting sources</li> </ul>"},{"location":"data-transformation/dbt/#testing","title":"Testing","text":"<ul> <li>What is testing?</li> <li>Generic tests</li> </ul>"},{"location":"data-transformation/dbt/#day-3","title":"Day 3","text":"<p>Let refresh our memory on data layers for intermediate models. Let's also revisit common table expressions (CTEs)</p>"},{"location":"data-transformation/dbt/#cte-examples","title":"CTE examples","text":"<pre><code>-- Let's go from writing our code like this...\n\nselect\n    \"station_id\",\n    \"latitude\",\n    \"longitude\",\n    \"county_name\"\n\nfrom {{ source('WATER_QUALITY', 'lab_results') }}\n\n-- To writing our code like this\n\nwith source as (\n    select * from {{ source('WATER_QUALITY', 'lab_results') }}\n),\n\nlab_results as (\n  select\n    \"station_id\",\n    \"latitude\",\n    \"longitude\",\n    \"county_name\"\n\n  from source\n)\n\nselect * from lab_results\n</code></pre> <p>Here\u2019s another example of a more complex, multi-stage CTE query.</p>"},{"location":"data-transformation/dbt/#materializations","title":"Materializations","text":"<p>Materializations refer to the way dbt executes and persists the results of SQL queries. It is the Data Definition Language (DDL) and Data Manipulation Language (DML) used to create a model\u2019s equivalent in a data warehouse.</p> <p>Understanding the options for materializations will allow you to choose the best strategy based on factors like query performance, data freshness, and data volume. There are four materializations used in dbt: view, table, incremental, and ephemeral. We used dbt docs as our main source for most of the materialization descriptions below.</p>"},{"location":"data-transformation/dbt/#view","title":"View","text":"<ul> <li>Views return the freshest, real-time state of their input data when they\u2019re queried, this makes them ideal as building blocks for larger models.</li> <li>Views are also great for small datasets with minimally intensive logic that we want near real time access to.</li> <li>Staging models are rarely accessed directly by our end users.</li> <li>Staging models need to be always up-to-date and in sync with our source data as building blocks for later models so we\u2019ll want to materialize our staging models as views.</li> <li>Since views are the default materialization in dbt, we don\u2019t have to do any specific configuration for this.</li> <li>Still, for clarity, it\u2019s a good idea to go ahead and specify the configuration to be explicit. We\u2019ll want to make sure our dbt_project.yml looks like this:</li> </ul> <pre><code>models:\n  jaffle_shop:\n    staging:\n      +materialized: view\n</code></pre>"},{"location":"data-transformation/dbt/#table","title":"Table","text":"<ul> <li>Tables are the most performant materialization, they return transformed data when queried with no need for reprocessing.</li> <li>Tables are also ideal for frequently used, compute intensive transformations. Making a table allows us to freeze transformations in place.</li> <li>Marts, like one that services a popular dashboard, are frequently accessed directly by our end users, and need to be performant.</li> <li>Can often function with intermittently refreshed data, end user decision making in many domains is fine with hourly or daily data.</li> <li>Given the above properties we\u2019ve got a great use case for building the data itself into the warehouse, not the logic. In other words, a table.</li> <li>The only decision we need to make with our marts is whether we can process the whole table at once or do we need to do it in chunks, that is, are we going to use the table materialization or incremental.</li> </ul>"},{"location":"data-transformation/dbt/#incremental","title":"Incremental","text":"<ul> <li>Incremental models build a table in pieces over time, only adding and updating new or changed rows.</li> <li>Builds more quickly than a regular table of the same logic.</li> <li>Initial runs are slow. Typically we use incremental models on very large datasets, so building the initial table on the full dataset is time consuming and equivalent to the table materialization.</li> </ul> <p>Sources: Incremental models in-depth and Available materializations</p>"},{"location":"data-transformation/dbt/#a-comparison-table","title":"A comparison table","text":"<p>Source: Available materializations</p>"},{"location":"data-transformation/dbt/#materializations-golden-rule","title":"Materializations golden rule","text":"<ul> <li>\ud83d\udd0d Start with a view. When the view gets too long to query for end users,</li> <li>\u2692\ufe0f Make it a table. When the table gets too long to build in your dbt Jobs,</li> <li>\ud83d\udcda Build it incrementally. That is, layer the data in chunks as it comes in.</li> </ul>"},{"location":"data-transformation/dbt/#ephemeral","title":"Ephemeral","text":"<p>Ephemeral models are not directly built into the database. Instead, dbt will interpolate the code from this model into dependent models as a CTE. Use the ephemeral materialization for:</p> <ul> <li>Light-weight transformations that are early on in your DAG</li> <li>When they are only used in one or two downstream models, and</li> <li>Do not need to be queried directly</li> </ul> <p>\u2705 Can help keep your data warehouse clean by reducing clutter \ud83d\udeab Overuse of ephemeral materialization can make queries harder to debug</p> <p>Source: Materializations</p>"},{"location":"data-transformation/dbt/#where-to-configure-materializations","title":"Where to configure materializations","text":"<p>You can configure models in <code>dbt_project.yml</code>, the YAML file within the corresponding model\u2019s folder, or within a specific model itself. Confusing thing about dbt configuration: the syntax and format change depend on where you use it!</p> <pre><code># in the dbt_project.yml file...\nmodels:\n  dse_analytics:\n    staging:\n      +materialized: view\n    intermediate:\n      +materialized: view\n    marts:\n      +materialized: table\n\n# the YAML file within the corresponding model\u2019s folder\nversion: 2\n\nmodels:\n  - name: int_state_entities__active\n    materialized: table\n    description: This is a sample description.\n    columns:\n      - name: name\n        description: Name of the entity\n</code></pre> <pre><code>-- within a specific model itself\n{{\n    config(\n        materialized='view'\n    )\n}}\n\nselect ...\n</code></pre>"},{"location":"data-transformation/dbt/#day-3-practice","title":"Day 3: Practice","text":"<p>Click either link for dbt Cloud or dbt Core practice.</p>"},{"location":"data-transformation/dbt/#day-3-references","title":"Day 3: References","text":""},{"location":"data-transformation/dbt/#materializations-jinja-and-dbt-models","title":"Materializations, Jinja, and dbt Models","text":"<ul> <li>dbt materialization and performance considerations</li> <li>Jinja tutorial: Use Jinja to improve your SQL code</li> <li>Re-watch the second and third video from Day 1: Models in dbt</li> </ul>"},{"location":"data-transformation/dbt/#day-4","title":"Day 4","text":""},{"location":"data-transformation/dbt/#dbt-docs","title":"dbt Docs","text":"<p>A key feature of dbt is the automated generation of documentation and lineage from your project. The framework reads your SQL models and YAML configuration files and produces a static HTML document from them. This documentation can then be hosted in a number of places, including dbt Cloud, GitHub Pages, or Azure Static Web Apps. Depending on platforms we are using for the project, we will demonstrate dbt docs using one of the following:</p> <ol> <li>Hosted in dbt Cloud. This can be useful if you are writing docs in a branch and want to visualize how they are rendered.</li> <li>If we are using GitHub, we\u2019ll demonstrate how the docs can be built from the repository and hosted on GitHub Pages.</li> <li>If we are using Azure DevOps, we\u2019ll demonstrate how the docs can be built from the repository and hosted using Azure Static Web Apps.</li> <li>If we are using Bitbucket and are okay with public-facing docs, we'll demonstrate how the docs can be built and hosted using Bitbucket Cloud.</li> <li>If we are using none of the above, we'll show you how to generate and view the docs locally.</li> </ol>"},{"location":"data-transformation/dbt/#data-environments-and-jobs","title":"Data environments and jobs","text":""},{"location":"data-transformation/dbt/#environments-in-snowflake","title":"Environments in Snowflake","text":"<p>We often talk about the concept of \"environments\". Broadly speaking, environments are a collection of compute resources, software, and configuration, which together represent a functioning context for development. Examples of environments include:</p> <ol> <li>A \u201cproduction\u201d environment which is used to run the dbt models that have been merged to <code>main</code>. This can be run on an ad-hoc basis, or can be run on a schedule to ensure that models are never more than some amount of time old.</li> <li>A \"development\" environment, which is used to run tests on branches and pull requests, and can help to catch bugs and regressions before they are deployed to production.</li> <li>A \"user acceptance testing (UAT)\" environment, which can be used as a final testing environment for verifying code before it is deployed to production.</li> </ol> <p>Unfortunately, that's pretty vague, since there are lots of different ways environments can be set up! Depending on your situation, different environments in Snowflake could be represented by:</p> <ul> <li>entirely different accounts</li> <li>different databases within the same account, or even</li> <li>different schemas within the same database.</li> </ul> <p>In our default MDSA architecture we usually have two environments, \"dev\" and \"prod\", which reside in the same Snowflake account. Each of these environments consists of a set of databases corresponding to our layered data architecture (see our Snowflake training for more detail).</p>"},{"location":"data-transformation/dbt/#environments-in-dbt-cloud","title":"Environments in dbt Cloud","text":"<p>dbt Cloud also has a concept of an Environment, which is a virtual machine in dbt Cloud that has all of the relevant software dependencies and environment variables set. Roughly speaking, an environment in dbt Cloud will correspond to one of your environments in Snowflake.</p> <p>You\u2019ve already encountered one environment, which is your \u201cDevelop:  Cloud IDE\u201d. But you can create other environments in dbt Cloud for various purposes. Our typical dbt Cloud setup includes the following environments:</p> <ul> <li>Development, which uses the \"dev\" Snowflake environment. This is what you use when you work in the cloud IDE.</li> <li>Production, which uses the \"prod\" Snowflake environment. This is what we use to build production data models.</li> <li>Continuous Integration, which uses the \"dev\" Snowflake environment. This is what runs the automated CI checks.</li> </ul>"},{"location":"data-transformation/dbt/#jobs","title":"Jobs","text":"<p>A \"job\" is a command or series of commands that run in a given environment. Examples of jobs we often use in our MDSA projects include:</p> <ul> <li>Running a nightly build of data models</li> <li>Running continuous integration (CI, see below!) checks</li> <li>Building project docs</li> </ul> <p>Jobs can be configured in a number of ways: they can have different environment variables set, they can run on a schedule, or they can be triggered by a specific action like a pull request being opened, or a branch being merged.</p>"},{"location":"data-transformation/dbt/#continuous-integration-and-continuous-deployment-cicd","title":"Continuous integration and continuous deployment (CI/CD)","text":""},{"location":"data-transformation/dbt/#what-continuous-integration-ci-is-and-why-you-shouldnt-ignore-it","title":"What Continuous Integration (CI) is, and why you shouldn\u2019t ignore it","text":"<p>Continuous Integration checks in GitHub, Azure DevOps, or BitBucket are automated tests that are run against your code every time you push a change. They are an important part of the software development process, and can help you:</p> <ul> <li>Catch errors and issues early: CI checks can identify issues with your code before they can cause problems in production.</li> <li>Improve code quality: CI checks can help you to improve the quality of your code by identifying issues like duplicate or dead code and potential security vulnerabilities.</li> <li>Establish a house style: CI checks can enforce various code formatting rules and conventions that your team has agreed upon.</li> </ul> <p>We have set up your project repository so that PRs cannot be merged to <code>main</code> unless these checks pass. This can sometimes feel annoying! At the end of the day, however, CI/CD checks shouldn\u2019t feel too painful or like a box-checking exercise: they are rather intended to be a routine and helpful part of the development process. Ultimately, experience has shown that effective use of CI/CD greatly speeds up development.</p>"},{"location":"data-transformation/dbt/#continuous-deployment-cd","title":"Continuous Deployment (CD)","text":"<p>Continuous Deployment (CD) in most MDSA projects is usually pretty simple. We typically do not build any applications or deploy cloud resources. Instead, whatever is in the <code>main</code> branch is considered \"production\", and our dbt projects and docs are built using that.</p> <p>For a deeper dive into how CI/CD is configured for this project see these docs</p>"},{"location":"data-transformation/dbt/#demo-cicd-in-a-development-workflow","title":"Demo: CI/CD in a development workflow","text":"<ol> <li>How to read the results of CI checks on a PR.</li> <li>How merging to <code>main</code> results in production dbt builds.</li> </ol>"},{"location":"data-transformation/dbt/#custom-schema-names","title":"Custom schema names","text":"<ol> <li>We\u2019ll talk about how the database schemas in which dbt models are built are determined. In development, the models get built in a different place (e.g., your <code>DBT_&lt;first-name-initial+last-name&gt;</code>schema) than they do in production.</li> <li>We\u2019ll discuss how this project is configured to use a custom schema name generated using <code>transform/macros/get_custom_schema.sql</code>.</li> </ol>"},{"location":"data-transformation/dbt/#day-4-practice","title":"Day 4: Practice","text":"<p>Click either link for dbt Cloud or dbt Core practice.</p>"},{"location":"data-transformation/dbt/#day-4-references","title":"Day 4: References","text":""},{"location":"data-transformation/dbt/#documentation","title":"Documentation","text":"<ul> <li>What is documentation?</li> <li>Writing documentation and doc blocks</li> <li>Creating encrypted key-pairs for service accounts that run CI and Production jobs</li> </ul>"},{"location":"data-transformation/advanced/macros-custom-tests/","title":"Macros and custom tests in dbt","text":""},{"location":"data-transformation/advanced/macros-custom-tests/#macros","title":"Macros","text":"<p>Macros in Jinja are pieces of code that can be reused multiple times \u2013 they are analogous to \"functions\" in other programming languages, and are extremely useful if you find yourself repeating code across multiple models. Remember (DRY). Macros are defined in .sql files, typically in your macros directory (e.g.<code>transform/macros</code>).</p> <p>Take a look at our macro example on the <code>stations</code> data.</p> <ol> <li>Switch to the <code>britt-dbt-training</code> branch</li> <li>Open and review <code>transform/macros/map_county_name_to_county_fips.sql</code><ol> <li>Then open <code>transform/models/intermediate/training/int_water_quality__counties.sql</code> and review line 9</li> </ol> </li> <li>Review this second macro example that is called by this code</li> </ol>"},{"location":"data-transformation/advanced/macros-custom-tests/#dbt_utils-package","title":"dbt_utils package","text":"<p>The dbt_utils package contains macros that can be (re)used across this project. Software engineers frequently modularize code into libraries. These libraries help programmers operate with leverage. In dbt, libraries like these are called packages. dbt's packages are powerful because they tackle many common analytic problems that are shared across teams.</p> <ol> <li>Example that uses the dbt_utils test equal_rowcount<ol> <li>Switch to the <code>britt-dbt-training</code> branch</li> <li>Open <code>transform/models/staging/training/_water_quality.yml</code> and review lines 6-10</li> <li>Run<code>dbt test --select stg_water_quality__stations</code><ol> <li>Note this test fails if trying to compare a table to a view</li> </ol> </li> </ol> </li> </ol> <pre><code>version: 2\n\nmodels:\n  - name: stg_water_quality__stations\n    description: Staging model for stations.\n    config:\n      materialized: table\n    data_tests:\n      - dbt_utils.equal_rowcount:\n        compare_model: source('WATER_QUALITY', 'stations')\n</code></pre>"},{"location":"data-transformation/advanced/macros-custom-tests/#data-tests","title":"Data tests","text":"<p>Let's revisit data tests from our foundational dbt training!</p>"},{"location":"data-transformation/advanced/macros-custom-tests/#custom-tests","title":"Custom tests","text":"<p>dbt allows you to create custom tests if you cannot find what you\u2019re looking for with the generic tests, the dbt_utils package (see above), or in other packages. Custom tests are assertions you make about your models and other resources in your dbt project (e.g. sources, seeds, etc.). When you run <code>dbt test</code>, dbt will tell you if each test in your project passes or fails. You can use open source dbt packages like dbt_utils (mentioned above) or dbt_expectations rather than reinventing the wheel!</p> <p>Custom tests are stored in the <code>tests</code> directory or <code>test-paths</code> entry in your <code>dbt_project.yml</code></p> <p>There are two ways of defining custom tests in dbt:</p> <ol> <li> <p>A singular data test is testing in its simplest form: If you can write a SQL query that returns failing rows, you can save that query in a <code>.sql</code> file within your test directory. It's now a data test, and it will be executed by the dbt test command.</p> <p>Definition:</p> <ul> <li>SQL file with a select-type query</li> <li>Jinja macros are usable, like ref() and source()</li> </ul> <p>How to execute: Test will automatically run on <code>dbt test</code> or <code>dbt build</code> if it is in the correct directory</p> <p>dbt example:</p> <pre><code>~/repo/transform $ cat tests/singular/example.sql\nSELECT value\nFROM (SELECT 1 AS value FROM {{ ref('model_name') }})\nWHERE value = 0\n\n~/repo/transform $ dbt test --select example\n22:24:57  Running with dbt=1.6.10\n22:24:58\n22:24:58  1 of 1 START test example ............ [RUN]\n22:24:58  1 of 1 PASS example .................. [PASS in 0.64s]\n22:24:58\n22:24:58  Finished running 1 test in 1.32 seconds (1.32s).\n22:24:58\n22:24:58  Completed successfully\n22:24:58\n22:24:58  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 TOTAL=1\n</code></pre> </li> <li> <p>A generic data test is a parameterized query that accepts arguments. The test query is defined in a special test block (like a macro). Once defined, you can reference the generic test by name throughout your <code>.yml</code> files\u2014define it on models, columns, sources, seeds, etc.</p> <p>Definition:</p> <ul> <li>Reusable test macro that accepts arguments to generate a query</li> </ul> <p>How to use:</p> <ul> <li>Define test (as a macro) in a file</li> <li>Associate test with models</li> <li>Add additional arguments as needed</li> </ul> <p>dbt example:</p> <pre><code>~/repo/transform $ cat models/_models.yml\nmodels:\n  - name: city_county_extras\n    columns:\n      - name: county\n        data_tests:\n          - valid_county\n</code></pre> <pre><code>~/repo/transform $ cat tests/generic/test_valid_county.sql\n{% test valid_county(model, column_name) %}\n\nSELECT {{ column_name }}\nFROM {{ model }}\nWHERE {{ column_name }} NOT IN\n(SELECT name FROM {{ ref('stg_tiger_counties') }})\n\n{% endtest %}\n</code></pre> </li> </ol>"},{"location":"data-transformation/advanced/macros-custom-tests/#practice","title":"Practice","text":"<p>We wrote a test to verify that the <code>sample_date</code> column in the <code>stg_water_quality__lab_results</code> model matches this regex <code>\\d{4}-\\d{2}-\\d{2}</code>.</p> <p>To run the test yourself:</p> <ol> <li>Check out the branch: <code>britt-dbt-training</code></li> <li> <p>Navigate to the file: <code>transform/tests/singular/verify_sample_date.sql</code>, you should see the following code:</p> <pre><code>with validation as (\nselect\n    sample_date,\n    case\n        when REGEXP_LIKE(sample_date, '^\\\\d{4}-\\\\d{2}-\\\\d{2}$') then 0\n        else 1\n    end as verify_format\nfrom {{ ref('stg_water_quality__lab_results') }}\n)\n\nselect verify_format\nfrom validation\ngroup by verify_format\nhaving sum(verify_format) &gt; 0\n</code></pre> </li> <li> <p>Run the test <code>dbt test --select verify_sample_date</code></p> </li> <li>The test should pass!</li> </ol>"}]}